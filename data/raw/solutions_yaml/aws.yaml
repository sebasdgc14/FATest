- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of terraform-provider-aws for managing AWS resources with Terraform
  need: Enforce restricted access between network segments in AWS
  solution:
    insecure_code_example:
      description: |-
        The above Terraform code defines an AWS security group that allows unrestricted access to and from all IP addresses (0.0.0.0/0) on all ports (from_port = 0, to_port = 0) and for all protocols (protocol = "-1"). This means that any machine, regardless of its location or network, can connect to the resources protected by this security group, and vice versa. This can lead to a potential security risk as it exposes the resources to unwanted traffic and potential attacks.

        The `ingress` block defines the inbound rules for connections to the resources, while the `egress` block defines the outbound rules for connections from the resources. The `cidr_blocks` attribute specifies the range of IP addresses that can connect to or from the resources. In this case, the range is set to `0.0.0.0/0`, which represents all possible IP addresses.

        The `from_port` and `to_port` attributes define the range of ports that can be used for the connections. When both are set to `0`, it means all ports are allowed. The `protocol` attribute defines the network protocol for the connections, and when it is set to `-1`, it means all protocols are allowed.

        This configuration is too permissive and goes against the principle of least privilege, which states that a user or system should have the minimum levels of access necessary to perform its function. Therefore, it's recommended to restrict the access to only the necessary IP addresses, ports, and protocols.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_security_group" "allow_all" {
          name        = "allow_all"
          description = "Allow all inbound traffic"

          ingress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }

          egress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The original code had a security group that allowed all inbound and outbound traffic, which is a significant security risk. The updated code restricts access to only the necessary network segments, ports, IP addresses, network protocols, and administrative services.

        The `ingress` block defines the inbound rules for network traffic. The `from_port` and `to_port` specify the port range for the rule, the `protocol` specifies the protocol type for the rule, and the `cidr_blocks` specify the range of source IP addresses for the rule.

        Similarly, the `egress` block defines the outbound rules for network traffic. The `from_port` and `to_port` specify the port range for the rule, the `protocol` specifies the protocol type for the rule, and the `cidr_blocks` specify the range of destination IP addresses for the rule.

        By limiting the access to only what's necessary, we significantly reduce the potential attack surface. It's important to regularly review and update these rules to ensure they meet the application's requirements and to identify any potential misconfigurations or unauthorized access.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_security_group" "restricted_access" {
          name        = "restricted_access"
          description = "Allow only necessary inbound and outbound traffic"

          ingress {
            from_port   = <required_from_port>
            to_port     = <required_to_port>
            protocol    = "<required_protocol>"
            cidr_blocks = ["<required_inbound_cidr_blocks>"]
          }

          egress {
            from_port   = <required_from_port>
            to_port     = <required_to_port>
            protocol    = "<required_protocol>"
            cidr_blocks = ["<required_outbound_cidr_blocks>"]
          }
        }
    steps:
      - Review the current security group configuration.
      - Identify the network segments, ports, IP addresses, network protocols, and administrative
        services that are required for the application.
      - Update the security group configuration to only allow access from the necessary
        network segments, ports, IP addresses, network protocols, and administrative
        services.
      - Remove the 'allow_all' rule that allows unrestricted access.
      - Test the updated security group configuration to ensure it meets the application's
        requirements.
      - Monitor and regularly review the security group configuration to identify any
        potential misconfigurations or unauthorized access.
  title: Unrestricted access between network segments - AWS
  vulnerability_id: "024"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Restriction of privileges to the minimum necessary level
  solution:
    insecure_code_example:
      description: |-
        The above Terraform code creates an IAM role (`aws_iam_role`) and attaches a policy (`aws_iam_role_policy`) to it. The IAM role is allowed to assume the role from any EC2 service.

        The vulnerability lies in the policy attached to the IAM role. The policy allows any action (`"Action": "*"`) on any resource (`"Resource": "*"`) in AWS. This means the role has excessive privileges and can perform any action on any resource in AWS.

        This is a security risk as if the role's credentials are compromised, the attacker can perform any action on any resource, potentially leading to unauthorized access, data leakage, or even a complete takeover of the AWS account.

        The principle of least privilege is not followed here. The principle of least privilege recommends that a user, program or system process should have the minimal privileges necessary to complete its job. Overprivileged roles or users can lead to misuse, whether intentional (malicious insiders) or accidental (misuse of permissions).
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_role" "iam_role" {
          name = "iam_role"
          assume_role_policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "sts:AssumeRole",
              "Principal": {
                "Service": "ec2.amazonaws.com"
              },
              "Effect": "Allow",
              "Sid": ""
            }
          ]
        }
        EOF
        }

        resource "aws_iam_role_policy" "iam_role_policy" {
          name = "iam_role_policy"
          role = "${aws_iam_role.iam_role.id}"

          policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "*",
              "Resource": "*",
              "Effect": "Allow"
            }
          ]
        }
        EOF
        }
    language: aws
    secure_code_example:
      description:
        "The original code had a vulnerability where the IAM role had excessive\
        \ privileges. The IAM role policy allowed all actions (`\"Action\": \"*\"\
        `) on all resources (`\"Resource\": \"*\"`). This is a security risk as it\
        \ violates the principle of least privilege, which states that a user should\
        \ have only the privileges necessary to perform their job.\n\nThe fixed code\
        \ reduces the permissions of the IAM role to only what is necessary. The IAM\
        \ role policy now only allows specific EC2 actions (`\"Action\": [\"ec2:Describe*\"\
        , \"ec2:StartInstances\", \"ec2:StopInstances\"]`) on specific EC2 instances\
        \ (`\"Resource\": [\"arn:aws:ec2:us-west-2:123456789012:instance/*\"]`). \n\
        \nThis ensures that the IAM role cannot perform any actions or access any\
        \ resources that it does not need to, reducing the potential damage if the\
        \ role's credentials were to be compromised."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_role" "iam_role" {
          name = "iam_role"
          assume_role_policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "sts:AssumeRole",
              "Principal": {
                "Service": "ec2.amazonaws.com"
              },
              "Effect": "Allow",
              "Sid": ""
            }
          ]
        }
        EOF
        }

        resource "aws_iam_role_policy" "iam_role_policy" {
          name = "iam_role_policy"
          role = "${aws_iam_role.iam_role.id}"

          policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": ["ec2:Describe*", "ec2:StartInstances", "ec2:StopInstances"],
              "Resource": ["arn:aws:ec2:us-west-2:123456789012:instance/*"],
              "Effect": "Allow"
            }
          ]
        }
        EOF
        }
    steps:
      - Review the permissions required by the application, user, or role.
      - Identify the specific actions and resources that are necessary for the application
        to function properly.
      - Remove the 'Allow' statement with the wildcard (*) in the 'aws_iam_role_policy'
        resource.
      - Replace the wildcard (*) with the specific actions and resources required by
        the application.
      - Ensure that the permissions assigned to the role follow the principle of least
        privilege.
      - Test the application to verify that it functions correctly with the updated
        permissions.
  title: Excessive privileges - AWS
  vulnerability_id: "031"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of terraform-provider-aws for managing AWS resources using Terraform
  need: Protection of sensitive business information in AWS
  solution:
    insecure_code_example:
      description:
        "The above Terraform script contains a serious security vulnerability.\
        \ The AWS `access_key` and `secret_key` are hard-coded in plain text directly\
        \ in the script. This is a major risk as it exposes these sensitive credentials\
        \ to anyone who has access to the source code. \n\nThese credentials are essentially\
        \ the keys to the AWS kingdom. They can be used to access and control AWS\
        \ resources, potentially leading to unauthorized access, data leaks, and other\
        \ security incidents.\n\nMoreover, if the source code is version controlled\
        \ using a system like Git and is accidentally pushed to a public repository,\
        \ these credentials could be exposed to the outside world. This could lead\
        \ to disastrous consequences, including data breaches and substantial financial\
        \ costs.\n\nTherefore, it is highly recommended to never hard-code sensitive\
        \ credentials directly in the source code. Instead, use secure methods of\
        \ managing secrets, such as environment variables or AWS Secrets Manager."
      text: |-
        provider "aws" {
          access_key = "AKIAIOSFODNN7EXAMPLE"
          secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
          region     = "us-west-2"
        }
    language: aws
    secure_code_example:
      description: |-
        The AWS provider block has been modified to remove the hardcoded AWS access key and secret key. This is a critical change to prevent the exposure of these sensitive credentials which could lead to unauthorized access and potential data breaches.

        The AWS credentials (access key and secret key) should not be hardcoded in the Terraform scripts or any source code. Instead, they should be securely stored and accessed through secure means such as AWS Secrets Manager or environment variables.

        In addition, proper access controls should be implemented to restrict who can access these AWS credentials. This could include IAM policies, security groups, and network access control lists (NACLs).

        It is also recommended to regularly rotate these AWS credentials. This means generating a new set of AWS credentials and replacing the old ones. This can help to minimize the risk of exposure if the credentials are somehow compromised.

        Finally, monitoring and logging access to these AWS credentials can help to detect any unauthorized access attempts. This could include CloudTrail logs, CloudWatch alarms, and third-party security tools.

        By following these best practices, you can help to secure your AWS resources and protect your business information.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }
    steps:
      - Remove the AWS access key and secret key from the source code.
      - Store the AWS credentials securely, such as using AWS Secrets Manager or environment
        variables.
      - Implement proper access controls to restrict access to the AWS credentials.
      - Regularly rotate the AWS credentials to minimize the risk of exposure.
      - Monitor and log access to the AWS credentials to detect any unauthorized access
        attempts.
  title: Business information leak - AWS
  vulnerability_id: "224"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Secure storage of AWS credentials
  solution:
    insecure_code_example:
      description: |-
        The above Terraform code is used to configure the AWS provider. The `access_key` and `secret_key` are the AWS credentials which are required to authenticate and authorize the operations performed by the Terraform AWS provider.

        The vulnerability lies in the fact that these AWS credentials are stored in plain text within the source code. This is a security risk as anyone who can access this code will also be able to access these credentials. This could potentially lead to unauthorized access to the AWS resources.

        Furthermore, if this code is version controlled and pushed to a public repository, these credentials can be exposed to the public. This could lead to serious security breaches including data theft, unauthorized changes to the infrastructure, or incurring unexpected costs.
      text: |-
        provider "aws" {
          region     = "us-west-2"
          access_key = "AKIAIOSFODNN7EXAMPLE"
          secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
        }
    language: aws
    secure_code_example:
      description: |-
        The original code contained a vulnerability where AWS credentials were stored in plain text within the source code. This is a serious security risk as it exposes sensitive information that could be exploited by malicious actors.

        The updated code removes the plain text AWS access key and secret key from the source code. Instead, it retrieves these values from Terraform variables `var.access_key` and `var.secret_key`. These variables should be securely stored and retrieved from a secure location such as AWS Secrets Manager or AWS Parameter Store.

        The AWS access key and secret key are sensitive information that should be encrypted according to the organization's policies and best practices for encryption and secure storage. They should be retrieved from the secure location at runtime, ensuring that they are not exposed in the source code or any other insecure location.

        This change ensures that the AWS credentials are securely managed and reduces the risk of them being exposed and potentially exploited.
      text: |-
        provider "aws" {
          region = "us-west-2"
          access_key = var.access_key
          secret_key = var.secret_key
        }
    steps:
      - Remove the plain text AWS access key and secret key from the source code.
      - Store the AWS access key and secret key in a secure location, such as AWS Secrets
        Manager or AWS Parameter Store.
      - Retrieve the AWS access key and secret key from the secure location at runtime.
      - Encrypt the sensitive information before storing or transmitting it.
      - Follow the organization's policies and best practices for encryption and secure
        storage of sensitive information.
  title: Non-encrypted confidential information - AWS
  vulnerability_id: "247"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for infrastructure as code provisioning and management
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Implementation of a robust and secure authentication mechanism for AWS
  solution:
    insecure_code_example:
      description:
        "The above Terraform code is used to set up an AWS provider without\
        \ any authentication mechanism. This is a significant security vulnerability\
        \ as it allows anyone to access and manipulate the AWS infrastructure.\n\n\
        In this scenario, the AWS provider is set to the `us-west-2` region but no\
        \ authentication details are provided. This means that Terraform will attempt\
        \ to use default credentials sources, including environment variables or default\
        \ AWS CLI configurations. If these are not securely managed or are absent,\
        \ it could lead to unauthorized access to the AWS resources.\n\nTerraform\
        \ supports several ways of providing these credentials (Environment variables,\
        \ Shared credentials file, etc.). The absence of these mechanisms in the code\
        \ means that the code is relying on the default AWS CLI configurations. If\
        \ these configurations are not properly secured, it can lead to a serious\
        \ security breach. \n\nAlso, if the credentials can be easily bypassed, it\
        \ poses another security risk. For example, if the AWS credentials are hardcoded\
        \ in the code and the code is shared publicly, anyone can use these credentials\
        \ to gain access to the AWS resources. \n\nOverall, the absence of a secure\
        \ authentication mechanism or the presence of an easily bypassable one in\
        \ the AWS configuration is a serious security vulnerability that needs to\
        \ be addressed."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }
    language: aws
    secure_code_example:
      description:
        "The original code was vulnerable due to the absence of an authentication\
        \ mechanism. The AWS provider was only configured with a region, but no access\
        \ key or secret key was provided. This means that anyone could potentially\
        \ interact with your AWS resources without any form of authentication.\n\n\
        The fixed code now includes an `access_key` and `secret_key` in the AWS provider\
        \ block. These are used to authenticate with AWS and ensure that only authorized\
        \ users can interact with your AWS resources. \n\nPlease replace `\"my-access-key\"\
        ` and `\"my-secret-key\"` with your actual AWS access key and secret key.\
        \ \n\n**Note:** Storing your AWS credentials in your Terraform configuration\
        \ is not recommended. Instead, consider using [Environment Variables](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#environment-variables)\
        \ or the [AWS Shared Credentials file](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#shared-credentials-file)\
        \ for more secure credential management.\n\nRemember to never expose your\
        \ `access_key` and `secret_key` in your public repositories, it's a security\
        \ risk. Always use secure and encrypted methods to store and retrieve your\
        \ keys. \n\nAfter implementing the authentication mechanism, ensure to test\
        \ it to verify its effectiveness. Regularly review and update the authentication\
        \ mechanism as needed to maintain the security of your AWS infrastructure."
      text: |-
        provider "aws" {
          region     = "us-west-2"
          access_key = "my-access-key"
          secret_key = "my-secret-key"
        }
    steps:
      - Choose an appropriate AWS authentication mechanism based on your business
        security requirements.
      - Configure the chosen authentication mechanism in the AWS provider block.
      - Ensure that the authentication mechanism is properly implemented and cannot
        be bypassed.
      - Test the authentication mechanism to verify its effectiveness.
      - Regularly review and update the authentication mechanism as needed to maintain
        the security of your AWS infrastructure.
  title: Authentication mechanism absence or evasion - AWS
  vulnerability_id: "241"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Secure communication channels for data transmission in AWS
  solution:
    insecure_code_example:
      description:
        "In the above Terraform code, we are creating an AWS instance in\
        \ the us-west-2 region. The AWS provider is being configured with access_key\
        \ and secret_key. However, this code doesn't implement any secure communication\
        \ protocols or cipher suites. \n\nThis means that all the communications between\
        \ the Terraform client and AWS endpoint are susceptible to Man-in-the-Middle\
        \ (MitM) attacks, where an attacker could potentially intercept the communications,\
        \ gaining unauthorized access to sensitive data, and possibly even modifying\
        \ the data in transit.\n\nIn addition, the identity of the remote end of the\
        \ connection (in this case, the AWS endpoint) is not being authenticated.\
        \ This could allow an attacker to impersonate the AWS endpoint, tricking the\
        \ Terraform client into sending sensitive data to an unauthorized endpoint.\
        \ \n\nThis is a serious security vulnerability, as it could lead to data breaches\
        \ and unauthorized access to AWS resources."
      text: |-
        provider "aws" {
          region = "us-west-2"
          access_key = "my-access-key"
          secret_key = "my-secret-key"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "example-instance"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code now uses variables for `access_key` and `secret_key` instead of hardcoding them directly into the provider block. This is a more secure practice as it prevents the keys from being exposed in the code. The keys can be provided through a secure method such as environment variables or AWS Secrets Manager.

        In addition, an `ebs_block_device` block has been added to the `aws_instance` resource with the `encrypted` argument set to `true`. This ensures that the EBS volume attached to the instance is encrypted, providing data-at-rest security.

        Please note that this code assumes that you are using a secure communication channel (HTTPS) with AWS services, and that you have enabled secure cipher suites and encryption protocols. Also, it is recommended to implement mutual authentication to verify the identity of the remote end of the connection.

        Remember to replace `var.access_key` and `var.secret_key` with your actual AWS access key and secret key, which should be stored securely and not hardcoded into your Terraform code.
      text: |-
        provider "aws" {
          region = "us-west-2"
          access_key = var.access_key
          secret_key = var.secret_key
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "example-instance"
          }

          ebs_block_device {
            encrypted = true
          }
        }
    steps:
      - Use HTTPS instead of HTTP for communication with AWS services
      - Enable encryption in transit by configuring SSL/TLS for the communication channel
      - Ensure that secure cipher suites and encryption protocols are used
      - Implement mutual authentication to verify the identity of the remote end of
        the connection
      - Avoid hardcoding access keys and secret keys in the code, instead use secure
        credential management solutions
  title: Use of an insecure channel - AWS
  vulnerability_id: "281"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with AWS services and resources
  need: Enhancement of traceability and logging in AWS instances
  solution:
    insecure_code_example:
      description: |-
        In the above Terraform code, we are creating an AWS EC2 instance, an Elastic Load Balancer (ELB) and an S3 bucket. However, there is no logging enabled for any of these resources.

        1. **EC2 Instance**: The EC2 instance is created using the `aws_instance` resource. The `ami` and `instance_type` properties are set, and a tag is assigned to the instance. However, there is no logging property set for this instance, which means that no log files are being created for this EC2 instance.

        2. **ELB**: The ELB is created using the `aws_elb` resource. It is set to listen on port 80, and is associated with three availability zones. However, like the EC2 instance, there is no logging property set for this ELB.

        3. **S3 Bucket**: The S3 bucket is created using the `aws_s3_bucket` resource. The bucket is set to private, but again, there is no logging enabled for this bucket.

        The absence of logging in these resources means that it is impossible to identify and trace malicious actions or anomalous behaviors. In addition, even if logging was enabled, the log files do not have a detailed level of information, which limits their usefulness.
      text: |-
        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "example-instance"
          }
        }

        resource "aws_elb" "example" {
          name               = "example-elb"
          availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]

          listener {
            instance_port     = 80
            instance_protocol = "http"
            lb_port           = 80
            lb_protocol       = "http"
          }
        }

        resource "aws_s3_bucket" "bucket" {
          bucket = "bucket"
          acl    = "private"
        }
    language: aws
    secure_code_example:
      description: |-
        The above code fixes the vulnerability by enabling logging for EC2, ELB, and S3 instances.

        1. For the EC2 instance, the `monitoring` property is set to `true`. This enables detailed monitoring, which provides additional instance metrics and logs.

        2. For the ELB instance, the `access_logs` block is added. This block enables access logs for the load balancer and specifies the S3 bucket where the logs will be stored.

        3. For the S3 bucket, the `logging` block is added. This block enables server access logging for the bucket. The logs are stored in the specified target bucket with the specified prefix.
      text: |-
        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "example-instance"
          }

          monitoring = true
        }

        resource "aws_elb" "example" {
          name               = "example-elb"
          availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]

          listener {
            instance_port     = 80
            instance_protocol = "http"
            lb_port           = 80
            lb_protocol       = "http"
          }

          access_logs {
            bucket  = "my-access-logs-bucket"
            enabled = true
          }
        }

        resource "aws_s3_bucket" "bucket" {
          bucket = "bucket"
          acl    = "private"

          logging {
            target_bucket = "my-log-bucket"
            target_prefix = "log/"
          }
        }
    steps:
      - Set the logging property for the EC2 instances to enable logging of activities
        and events.
      - Configure the ELB to enable access logs, which will capture information about
        incoming requests.
      - Enable server access logging for the S3 bucket to track access to the bucket
        and its objects.
  title: Traceability Loss - AWS
  vulnerability_id: "400"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Minimization of exposed resources in AWS
  solution:
    insecure_code_example:
      description:
        "The above Terraform code is provisioning three AWS resources:\
        \ an EC2 instance, an S3 bucket, and an RDS database instance. \n\nThe vulnerability\
        \ lies in the configuration of the S3 bucket and the RDS database instance.\n\
        \n1. For the S3 bucket, the `acl` attribute is set to `public-read`. This\
        \ means that anyone on the internet can read the content of this bucket, which\
        \ might not be necessary for the operation of the application and could expose\
        \ sensitive data.\n\n2. For the RDS database instance, the `publicly_accessible`\
        \ attribute is set to `true`. This means that the database can be accessed\
        \ from the internet, which expands the attack surface if it's not necessary\
        \ for the operation of the application.\n\nThis code does not follow the principle\
        \ of least privilege, i.e., only the minimum required access should be granted\
        \ for the operation of the application. Other unnecessary resources and permissions\
        \ should not be exposed as they could be potential vectors for attacks."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "example-instance"
          }
        }

        resource "aws_s3_bucket" "example" {
          bucket = "example"
          acl    = "public-read"

          tags = {
            Name = "example-bucket"
          }
        }

        resource "aws_db_instance" "example" {
          allocated_storage    = 5
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = true
        }
    language: aws
    secure_code_example:
      description: |-
        The original code had two main vulnerabilities:

        1. The S3 bucket was publicly readable. This is a potential security risk as it allows anyone to read the data in the bucket. The ACL for the bucket has been changed to "private" to prevent public access.

        2. The database instance was publicly accessible. This is a potential security risk as it allows anyone to access the database. The `publicly_accessible` attribute has been set to `false` to prevent public access.

        These changes ensure that only necessary resources are exposed, reducing the attack surface and improving the security of the application. It's also recommended to regularly audit the AWS infrastructure to identify and remove any unnecessary resources, implement least privilege access control to restrict access to necessary resources, and regularly update and patch the AWS infrastructure to mitigate any potential vulnerabilities.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "example-instance"
          }
        }

        resource "aws_s3_bucket" "example" {
          bucket = "example"
          acl    = "private"

          tags = {
            Name = "example-bucket"
          }
        }

        resource "aws_db_instance" "example" {
          allocated_storage    = 5
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = false
        }
    steps:
      - Identify the necessary resources for the operation of the application.
      - Remove or disable any unnecessary resources that are exposed.
      - Review the AWS configuration and ensure that only necessary resources are provisioned.
      - Regularly audit the AWS infrastructure to identify and remove any unnecessary
        resources.
      - Implement least privilege access control to restrict access to necessary resources.
      - Regularly update and patch the AWS infrastructure to mitigate any potential
        vulnerabilities.
  title: Automatic information enumeration - AWS
  vulnerability_id: "253"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Secure configuration of AWS services
  solution:
    insecure_code_example:
      description: |-
        The above Terraform code is used to create an AWS security group that allows all inbound and outbound traffic. This is a typical example of insecure service configuration in AWS.

        The `aws_security_group` resource block creates a new security group named "allow_all". The `ingress` block specifies the inbound rules for this security group. In this case, it allows traffic from any IP address (0.0.0.0/0) on any port (from 0 to 0) using any protocol (-1, which stands for all protocols).

        Similarly, the `egress` block specifies the outbound rules for this security group. It allows traffic to any IP address on any port using any protocol.

        This configuration is highly insecure because it exposes all the resources associated with this security group to the public internet. Any malicious actor can identify and try to access these resources, exploit vulnerabilities, or launch attacks.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_security_group" "allow_all" {
          name        = "allow_all"
          description = "Allow all inbound traffic"

          ingress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }

          egress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The above code is a secure configuration of an AWS service using Terraform. The `aws_security_group` resource is used to create a security group that allows specific inbound and outbound traffic.

        In the `ingress` block, the `from_port` and `to_port` are set to 22, which is the port for SSH. The `protocol` is set to "tcp", and the `cidr_blocks` is set to "203.0.113.0/24". This means that only the IP addresses within the range 203.0.113.0 to 203.0.113.255 are allowed to access the service via SSH.

        Similarly, in the `egress` block, the `from_port` and `to_port` are set to 80, which is the port for HTTP. The `protocol` is set to "tcp", and the `cidr_blocks` is set to "203.0.113.0/24". This means that the service is only allowed to send outbound traffic to the IP addresses within the range 203.0.113.0 to 203.0.113.255 via HTTP.

        This configuration implements the principle of least privilege by only granting the necessary permissions to the security group. It also helps to prevent unauthorized access by restricting the inbound and outbound traffic to specific IP addresses or ranges. Regular reviews and updates of the security group rules, as well as enabling logging and monitoring, can further enhance the security of the service.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_security_group" "specific_access" {
          name        = "specific_access"
          description = "Allow specific inbound and outbound traffic"

          ingress {
            from_port   = 22
            to_port     = 22
            protocol    = "tcp"
            cidr_blocks = ["203.0.113.0/24"]
          }

          egress {
            from_port   = 80
            to_port     = 80
            protocol    = "tcp"
            cidr_blocks = ["203.0.113.0/24"]
          }
        }
    steps:
      - Restrict the inbound traffic to only allow access from specific IP addresses
        or ranges.
      - Restrict the outbound traffic to only allow access to specific IP addresses
        or ranges.
      - Remove the 'allow_all' security group and create more specific security groups
        based on the required access.
      - Implement least privilege principle by granting only the necessary permissions
        to the security groups.
      - Regularly review and update the security group rules to ensure they align
        with the current requirements.
      - Enable logging and monitoring for the security groups to detect any unauthorized
        access attempts.
  title: Insecure service configuration - AWS
  vulnerability_id: "165"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Enabling secure service configuration for S3 buckets
  solution:
    insecure_code_example:
      description:
        "The above Terraform code creates an S3 bucket on AWS with the\
        \ name \"bucket-name\". The bucket is configured to be private, meaning only\
        \ the AWS account that created the bucket and users that have been explicitly\
        \ granted permissions can access it.\n\nHowever, the code is vulnerable due\
        \ to the lack of versioning configuration. The S3 bucket versioning feature\
        \ is not enabled. This feature allows you to preserve, retrieve, and restore\
        \ every version of every object in your bucket. This is useful in protecting\
        \ against both unintended user actions and application failures.\n\nWithout\
        \ versioning, if an object is deleted or overwritten, there's no way to recover\
        \ the previous state. This could lead to data loss in case of accidental deletion\
        \ or overwriting, or malicious actions from users. \n\nTherefore, this code\
        \ is considered insecure as it does not enable the VersioningConfiguration\
        \ resource for the S3 bucket."
      text: |-
        resource "aws_s3_bucket" "bucket" {
          bucket = "bucket-name"
          acl    = "private"

          tags = {
            Name        = "My bucket"
            Environment = "Dev"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code includes a `versioning` block within the `aws_s3_bucket` resource. This block is used to enable versioning for the S3 bucket. The `status` attribute within the `versioning` block is set to `Enabled`, which turns on versioning for the bucket.

        Versioning allows you to preserve, retrieve, and restore every version of every object in your Amazon S3 bucket. This makes it easier to recover from both unintended user actions and application failures. You can use versioning to preserve, retrieve, and restore every version of every object in your bucket. With versioning, you can easily recover from both unintended user actions and application failures.
      text: |-
        resource "aws_s3_bucket" "bucket" {
          bucket = "bucket-name"
          acl    = "private"

          versioning {
            status = "Enabled"
          }

          tags = {
            Name        = "My bucket"
            Environment = "Dev"
          }
        }
    steps:
      - Enable versioning for the S3 bucket
      - Update the 'aws_s3_bucket' resource configuration to include the 'versioning'
        block
      - Set the 'status' attribute in the 'versioning' block to 'Enabled'
  title: Insecure service configuration - Bucket
  vulnerability_id: "335"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services (AWS) services
  need: Protection of confidential information through encryption
  solution:
    insecure_code_example:
      description:
        "The above code is a typical example of a Terraform script that\
        \ is used to manage AWS infrastructure. The `access_key` and `secret_key`\
        \ are confidential information that is used to authenticate to AWS.\n\nHowever,\
        \ the confidential information (access_key and secret_key) is encoded in hexadecimal.\
        \ This means that anyone who has access to the script can easily decode the\
        \ hexadecimal values to plain text, thereby gaining unauthorized access to\
        \ the AWS infrastructure.\n\nThis is a serious security vulnerability because\
        \ it exposes the AWS infrastructure to potential attacks. An attacker can\
        \ use the decoded access_key and secret_key to gain full control over the\
        \ AWS infrastructure, which can lead to data loss, data theft, infrastructure\
        \ disruption, and other serious consequences. \n\nIn addition, the use of\
        \ hardcoded credentials in the script violates the principle of least privilege\
        \ and the best practices for secure coding. It is recommended to use a secure\
        \ method to store and retrieve confidential information, such as using AWS\
        \ Secrets Manager or environment variables."
      text: |-
        provider "aws" {
          access_key = "AKIAIOSFODNN7EXAMPLE"
          secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
          region     = "us-west-2"
        }
    language: aws
    secure_code_example:
      description: |-
        The original code had the AWS `access_key` and `secret_key` hardcoded into the Terraform provider configuration. This is a security vulnerability as anyone with access to the codebase could potentially gain unauthorized access to the AWS account.

        The updated code removes the hardcoded AWS credentials and instead uses Terraform variables (`var.access_key` and `var.secret_key`). These variables should be securely stored and retrieved from a secure location, such as AWS Secrets Manager, HashiCorp Vault, or environment variables.

        The `access_key` and `secret_key` should be encrypted using a strong encryption method and should be regularly rotated to minimize the risk of unauthorized access. Any access to these credentials should be monitored and logged to detect any suspicious activity.

        This approach ensures that the AWS credentials are not exposed in the codebase, enhancing the security of the AWS account.
      text: |-
        provider "aws" {
          access_key = var.access_key
          secret_key = var.secret_key
          region     = "us-west-2"
        }

        variable "access_key" {
          description = "AWS Access Key"
          type        = string
        }

        variable "secret_key" {
          description = "AWS Secret Key"
          type        = string
        }
    steps:
      - Replace the access_key and secret_key values with secure and encrypted credentials.
      - Use a secure method to store and retrieve the encrypted credentials.
      - Ensure that the encryption method used is strong and follows best practices.
      - Regularly rotate the access_key and secret_key to minimize the risk of unauthorized
        access.
      - Monitor and log any access to the credentials to detect any suspicious activity.
  title: Non-encrypted confidential information - Hexadecimal
  vulnerability_id: "378"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS IAM User for managing user access and permissions in AWS
    - Usage of aws_iam_user_login_profile for managing login profiles for AWS IAM users
  need: Implementation of a strong password policy
  solution:
    insecure_code_example:
      description:
        "In the above code snippet, we are creating an IAM user login profile\
        \ without enforcing any password policy. This means that the IAM user can\
        \ have a weak password, which is a security risk. The policy does not enforce\
        \ password length, complexity, or expiration, which are all recommended for\
        \ a strong credential policy. \n\nA weak password policy can lead to unauthorized\
        \ access if the password is easily guessed or brute-forced. It is recommended\
        \ to enforce a strong password policy that includes a minimum password length,\
        \ complexity requirements (including numbers, uppercase letters, lowercase\
        \ letters, and non-alphanumeric characters), and password expiration."
      text: |-
        resource "aws_iam_user_login_profile" "user" {
          user = "${aws_iam_user.user.name}"
          pgp_key = "keybase:some_person_that_exists"
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code now enforces a strong password policy for the AWS IAM user. Here's what each attribute does:

        - `user`: This is the IAM user's name.
        - `pgp_key`: This is the keybase username of the person who will initially get the password.
        - `password_length`: This sets the minimum password length to 14 characters. You can increase this number based on your security requirements.
        - `password_reset_required`: This forces the user to change their password the next time they log in.
        - `require_uppercase_characters`: This ensures that the password must contain at least one uppercase letter.
        - `require_lowercase_characters`: This ensures that the password must contain at least one lowercase letter.
        - `require_numbers`: This ensures that the password must contain at least one number.
        - `require_symbols`: This ensures that the password must contain at least one special character.

        This policy significantly improves the strength of the password, making it harder for attackers to guess or brute-force the password. It's also recommended to implement multi-factor authentication (MFA) for added security and to regularly educate users on the importance of strong passwords and the risks associated with weak passwords.
      text: |-
        resource "aws_iam_user_login_profile" "user" {
          user    = "${aws_iam_user.user.name}"
          pgp_key = "keybase:some_person_that_exists"
          password_length = 14
          password_reset_required = true
          require_uppercase_characters = true
          require_lowercase_characters = true
          require_numbers = true
          require_symbols = true
        }
    steps:
      - Update the credential policy to enforce strong password requirements.
      - Require the use of phrases instead of word-based passwords.
      - Implement a password complexity policy that includes a minimum length, a mix
        of uppercase and lowercase letters, numbers, and special characters.
      - Consider implementing multi-factor authentication (MFA) for added security.
      - Regularly educate users on the importance of strong passwords and the risks
        associated with weak passwords.
      - Periodically review and update the credential policy to align with industry
        best practices.
  title: Weak credential policy - Password strength
  vulnerability_id: "363"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_db_instance for managing and interacting with AWS RDS database instances
  need: Restrict access between network segments for RDS instances
  solution:
    insecure_code_example:
      description: |-
        The above Terraform code creates a publicly accessible AWS RDS instance without specifying a database subnet group. The `publicly_accessible` attribute is set to `true` which means that the database instance is accessible from the internet.

        This is a vulnerability because it allows unrestricted access between network segments. Any public or private subnet can connect to the RDS instance without any restrictions. This could potentially lead to unauthorized access to sensitive data stored in the database.

        The lack of a defined database subnet group means that the RDS instance is not associated with any particular subnet. This could lead to issues with network routing and access control as it is not clear which network rules apply to the database instance.

        To fix this vulnerability, a database subnet group should be defined and associated with the RDS instance. The `publicly_accessible` attribute should be set to `false` to ensure that the database instance is not publicly accessible.
      text: |-
        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = true
          skip_final_snapshot  = true
        }
    language: aws
    secure_code_example:
      description: |-
        The above code first creates a Database Subnet Group using the `aws_db_subnet_group` resource. The `subnet_ids` attribute should be replaced with the IDs of the subnets you want to include in the group.

        Then, it creates an RDS instance using the `aws_db_instance` resource. The `publicly_accessible` attribute is set to `false` to ensure that the RDS instance is not publicly accessible.

        The `db_subnet_group_name` attribute is set to the name of the Database Subnet Group created earlier. This ensures that the RDS instance is defined inside the Database Subnet Group, restricting access between network segments.

        Please note that you should replace the placeholders (`subnet-xxxxxxxx`, `subnet-yyyyyyyy`) with your actual subnet IDs. Also, ensure that the security group rules associated with the subnets restrict access to the RDS instance as per your requirements.
      text: |-
        resource "aws_db_subnet_group" "default" {
          name       = "main"
          subnet_ids = ["subnet-xxxxxxxx", "subnet-yyyyyyyy"]

          tags = {
            Name = "Main database subnet group"
          }
        }

        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = false
          skip_final_snapshot  = true
          db_subnet_group_name = aws_db_subnet_group.default.name
        }
    steps:
      - Create a Database Subnet Group in your AWS account.
      - Specify the Database Subnet Group in the RDS resource configuration.
      - Ensure that the RDS instance is not publicly accessible.
      - Review and update the security group rules to restrict access to the RDS instance.
  title: Unrestricted access between network segments - RDS
  vulnerability_id: "109"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need:
    Implementation of safeguards to prevent accidental or unauthorized deletion
    of databases in Amazon Web Services RDS.
  solution:
    insecure_code_example:
      description:
        This script uses Terraform to create an AWS RDS instance. However,
        the configuration is vulnerable due to the lack of protection against deletion.
        The `skip_final_snapshot` parameter is set to `true`, which means that no
        final DB snapshot will be created before the DB instance is deleted. If this
        instance is deleted, all data stored in it will be lost permanently, and recovery
        would not be possible.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          skip_final_snapshot  = true
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code includes three main changes to address the vulnerability:

        1. `deletion_protection = true`: This line of code enables deletion protection for the RDS instance. With deletion protection enabled, the database cannot be deleted by any user. This feature can only be disabled by modifying the database settings, providing an additional layer of security against accidental or malicious deletions.

        2. `skip_final_snapshot = false`: This line of code ensures that a final DB snapshot is created before the DB instance is deleted. This provides a recovery point to restore from later if the deletion was accidental.

        3. `backup_retention_period = 7`: This line of code sets the number of days for which automated backups are retained. Setting this to a positive number enables automated backups. With automated backups, AWS RDS takes a daily snapshot of your data and retains it for the specified number of days. This allows you to restore your database to any point in time within your retention period, providing a robust recovery strategy in case of data loss.

        Remember, it's also important to implement proper access controls and permissions for the RDS instance and regularly monitor and review the RDS instance for any unauthorized changes or deletions. This can be done through AWS IAM roles and policies, and AWS CloudTrail for monitoring and logging respectively.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          skip_final_snapshot  = false
          deletion_protection  = true
          backup_retention_period = 7
        }
    steps:
      - Enable deletion protection for the RDS instance
      - Add a backup and recovery strategy for the RDS instance
      - Implement proper access controls and permissions for the RDS instance
      - Regularly monitor and review the RDS instance for any unauthorized changes or
        deletions
  title: Lack of protection against deletion - RDS
  vulnerability_id: "256"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of terraform-provider-aws for managing AWS resources with Terraform
  need: Secure configuration of IAM services
  solution:
    insecure_code_example:
      description: |-
        The above Terraform configuration code is vulnerable because it exposes sensitive information in plain text. Specifically, the `access_key` and `secret_key` for the AWS provider are hard-coded into the Terraform configuration file. These keys are extremely sensitive as they can provide full access to the AWS account.

        Furthermore, an IAM user is created with a hardcoded path and name. The IAM access key is also created and associated with the IAM user. The encrypted secret key is then outputted, which could potentially be logged or stored insecurely.

        This configuration is not compliant with security standards such as ISO 27017 and NIST SP 800-144, as sensitive information should not be stored in plain text or uploaded to the cloud. This could lead to data leakage and unauthorized access to the AWS account.
      text: |-
        provider "aws" {
          region     = "us-west-2"
          access_key = "AKIAIOSFODNN7EXAMPLE"
          secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
        }

        resource "aws_iam_user" "lb" {
          name = "loadbalancer"
          path = "/system/"
        }

        resource "aws_iam_access_key" "lb" {
          user = aws_iam_user.lb.name
          pgp_key = "keybase:username"
        }

        output "secret" {
          value = aws_iam_access_key.lb.encrypted_secret
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code removes the `access_key` and `secret_key` from the AWS provider block. This is a critical change as hardcoding sensitive information like AWS access keys and secret keys in the code can lead to serious security vulnerabilities such as unauthorized access or data leakage.

        Instead of hardcoding, these keys should be stored in environment variables. The AWS provider block will automatically use the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables if they are set.

        To set these environment variables, you can use the following commands in your shell:

        ```bash
        export AWS_ACCESS_KEY_ID="youraccesskey"
        export AWS_SECRET_ACCESS_KEY="yoursecretkey"
        ```

        Remember to replace `"youraccesskey"` and `"yoursecretkey"` with your actual AWS access key and secret key.

        It's important to note that these environment variables should be properly protected and not exposed in the code repository. You should never commit your AWS access keys and secret keys to your code repository.

        For an additional layer of security, consider using a secret management service like AWS Secrets Manager or HashiCorp Vault to securely store and retrieve your AWS access keys and secret keys. These services provide a centralized location to manage secrets and protect sensitive data using encryption.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_user" "lb" {
          name = "loadbalancer"
          path = "/system/"
        }

        resource "aws_iam_access_key" "lb" {
          user = aws_iam_user.lb.name
          pgp_key = "keybase:username"
        }

        output "secret" {
          value = aws_iam_access_key.lb.encrypted_secret
        }
    steps:
      - Remove the access_key and secret_key from the provider block
      - Use environment variables to store the access key and secret key
      - Retrieve the access key and secret key from the environment variables in the
        code
      - Ensure that the environment variables are properly protected and not exposed
        in the code repository
      - Consider using a secret management service to securely store and retrieve the
        access key and secret key
  title: Insecure service configuration - IAM
  vulnerability_id: "334"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_security_group for managing security groups in AWS
  need: Restrict open ports to only necessary and authorized services
  solution:
    insecure_code_example:
      description:
        "The above Terraform code creates a security group that allows\
        \ inbound traffic from any IP address (`0.0.0.0/0`) to all ports (`0` to `65535`).\
        \ This is a security risk as it exposes all services running on the instances\
        \ associated with this security group to the public internet. \n\nAn attacker\
        \ can easily scan these ports to discover running services and exploit known\
        \ vulnerabilities in these services. This is an example of automatic information\
        \ enumeration vulnerability where open ports are scanned to gather information\
        \ about available services. \n\nThe egress block is also configured to allow\
        \ all outbound traffic from the instances, which could potentially be exploited\
        \ by an attacker who gains access to the instance to exfiltrate data or launch\
        \ attacks on other systems.\n\nThis configuration should be replaced with\
        \ a more restrictive one, allowing only necessary ports and IP ranges to reduce\
        \ the potential attack surface."
      text: |-
        resource "aws_security_group" "allow_all" {
          name        = "allow_all"
          description = "Allow all inbound traffic"

          ingress {
            from_port   = 0
            to_port     = 65535
            protocol    = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
          }

          egress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }
    language: aws
    secure_code_example:
      description:
        "The updated code modifies the `aws_security_group` resource to\
        \ restrict the inbound traffic to only the necessary ports and sources. \n\
        \nIn the `ingress` block, the `from_port` and `to_port` values have been updated\
        \ to `22`, which is the standard port for SSH. This means that only traffic\
        \ on port 22 will be allowed to enter. \n\nThe `cidr_blocks` value has been\
        \ set to `203.0.113.0/24`, which restricts the source IP addresses that are\
        \ allowed to access the port to this specific range. \n\nThe `egress` block\
        \ remains the same, allowing all outbound traffic. However, this should be\
        \ updated as per your application requirements to restrict outbound traffic\
        \ to only necessary ports and destinations.\n\nRemember to test the updated\
        \ security group configuration to ensure that only the necessary ports are\
        \ open and accessible. Also, monitor and regularly review the security group\
        \ configuration to identify and address any potential vulnerabilities."
      text: |-
        resource "aws_security_group" "allow_specific" {
          name        = "allow_specific"
          description = "Allow specific inbound traffic"

          ingress {
            from_port   = 22
            to_port     = 22
            protocol    = "tcp"
            cidr_blocks = ["203.0.113.0/24"]
          }

          egress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }
    steps:
      - Review the security requirements for the application and determine the necessary
        inbound and outbound traffic.
      - Modify the 'aws_security_group' resource configuration to restrict the inbound
        traffic to only the required ports and sources.
      - Update the 'from_port' and 'to_port' values in the 'ingress' block to specify
        the specific port range needed for the application.
      - Set the 'cidr_blocks' value in the 'ingress' block to limit the source IP
        addresses or ranges that are allowed to access the ports.
      - Update the 'egress' block to restrict outbound traffic as per the application
        requirements.
      - Test the updated security group configuration to ensure that only the necessary
        ports are open and accessible.
      - Monitor and regularly review the security group configuration to identify
        and address any potential vulnerabilities.
  title: Automatic information enumeration - Open ports
  vulnerability_id: "252"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_redshift_cluster for managing and interacting with Amazon Redshift
      clusters
  need: Encryption of confidential information in AWS Redshift Cluster
  solution:
    insecure_code_example:
      description:
        "This Terraform code block creates an Amazon Redshift cluster without\
        \ encryption. The `aws_redshift_cluster` resource block defines the properties\
        \ of the cluster. Here, `cluster_identifier` is a unique name for the cluster,\
        \ `database_name` is the name of the initial database to be created when the\
        \ cluster is created, `master_username` and `master_password` are the credentials\
        \ for the master database user, `node_type` is the type of node to be used\
        \ in the cluster, and `cluster_type` is the number of nodes in the cluster.\n\
        \nHowever, this code is vulnerable because it does not enable encryption for\
        \ the Redshift cluster. This means that all data stored in the cluster is\
        \ not encrypted, and if an unauthorized individual were to gain access to\
        \ this data, they could read it without needing any sort of encryption key.\
        \ This poses a serious security risk, particularly if the data stored in the\
        \ cluster is sensitive. \n\nTo protect the data in the Redshift cluster, it\
        \ should be encrypted using AWS Key Management Service (KMS). This can be\
        \ achieved by adding the `kms_key_id` property to the `aws_redshift_cluster`\
        \ resource block and setting it to the ARN of the KMS key to be used for encryption.\
        \ The `encrypted` attribute should also be set to `true`."
      text: |-
        resource "aws_redshift_cluster" "default" {
          cluster_identifier = "tf-redshift-cluster"
          database_name      = "mydb"
          master_username    = "foo"
          master_password    = "Mustbe8characters"
          node_type          = "dc1.large"
          cluster_type       = "single-node"
        }
    language: aws
    secure_code_example:
      description:
        "The above code fixes the vulnerability by enabling encryption\
        \ for the AWS Redshift cluster. \n\nThe `aws_kms_key` resource block creates\
        \ a new AWS KMS key that will be used to encrypt the Redshift cluster. The\
        \ policy attached to the key allows all KMS actions for the root user of the\
        \ AWS account.\n\nIn the `aws_redshift_cluster` resource block, the `encrypted`\
        \ attribute is set to `true` to enable encryption. The `kms_key_id` attribute\
        \ is set to the ARN of the newly created KMS key. This means that the Redshift\
        \ cluster will be encrypted using this key.\n\nThis ensures that all data\
        \ stored in the Redshift cluster is encrypted and can only be accessed by\
        \ entities that have the necessary permissions to use the KMS key. This significantly\
        \ reduces the risk of unauthorized access to the data."
      text: |-
        resource "aws_kms_key" "redshift" {
          description = "KMS key for redshift"
          policy      = <<POLICY
        {
          "Version": "2012-10-17",
          "Id": "key-default-1",
          "Statement": [
            {
              "Sid": "Enable IAM User Permissions",
              "Effect": "Allow",
              "Principal": {
                "AWS": "arn:aws:iam::${var.account_id}:root"
              },
              "Action": "kms:*",
              "Resource": "*"
            }
          ]
        }
        POLICY
        }

        resource "aws_redshift_cluster" "default" {
          cluster_identifier = "tf-redshift-cluster"
          database_name      = "mydb"
          master_username    = "foo"
          master_password    = "Mustbe8characters"
          node_type          = "dc1.large"
          cluster_type       = "single-node"
          encrypted          = true
          kms_key_id         = aws_kms_key.redshift.arn
        }
    steps:
      - Enable encryption for the AWS Redshift cluster
      - Modify the 'aws_redshift_cluster' resource to include the 'encrypted' attribute
      - Set the 'encrypted' attribute to 'true'
      - Ensure that the 'kms_key_id' attribute is also set to specify the AWS Key Management
        Service (KMS) key to use for encryption
  title: Non-encrypted confidential information - Redshift Cluster
  vulnerability_id: "433"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Secure encryption of confidential information in DynamoDB
  solution:
    insecure_code_example:
      description:
        "The vulnerable code above demonstrates a DynamoDB table being\
        \ created without any encryption specified. The `aws_dynamodb_table` resource\
        \ block does not include the `server_side_encryption` argument which is used\
        \ to enable server-side encryption using an AWS managed KMS key or a Customer\
        \ managed KMS key (CMK). \n\nBecause encryption is not specified, the DynamoDB\
        \ table will use the default AWS managed KMS key for server-side encryption.\
        \ This default key does not provide the same level of control and security\
        \ as a CMK. \n\nFor example, with a CMK, you can create, rotate, disable,\
        \ and define access controls, and you can audit the encryption keys used to\
        \ protect your data. Also, AWS does not have access to your CMKs, which means\
        \ the data is securely protected. \n\nTherefore, the code is vulnerable as\
        \ it does not use a CMK for encryption, potentially leading to unauthorized\
        \ access or data breaches."
      text: |-
        resource "aws_dynamodb_table" "basic-dynamodb-table" {
          name           = "GameScores"
          billing_mode   = "PROVISIONED"
          read_capacity  = 20
          write_capacity = 20
          hash_key       = "UserId"
          range_key      = "GameTitle"

          attribute {
            name = "UserId"
            type = "N"
          }

          attribute {
            name = "GameTitle"
            type = "S"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The above code first creates a Customer Managed Key (CMK) using the `aws_kms_key` resource. This key is then used to encrypt the DynamoDB table by specifying the `kms_key_arn` attribute in the `aws_dynamodb_table` resource.

        The `aws_kms_key` resource creates a new KMS key with a policy that allows all KMS actions for the root user of the AWS account. The ARN of the created key is then used in the `aws_dynamodb_table` resource to enable server-side encryption using the created CMK.

        This ensures that all data stored in the DynamoDB table is encrypted using the specified CMK, thereby fixing the vulnerability of non-encrypted confidential information.
      text: |-
        resource "aws_kms_key" "dynamodb" {
          description = "KMS key for DynamoDB"
          policy = <<POLICY
        {
          "Version": "2012-10-17",
          "Id": "key-consolepolicy-3",
          "Statement": [
            {
              "Sid": "Enable IAM User Permissions",
              "Effect": "Allow",
              "Principal": {"AWS": "arn:aws:iam::${var.account_id}:root"},
              "Action": "kms:*",
              "Resource": "*"
            }
          ]
        }

        resource "aws_dynamodb_table" "basic-dynamodb-table" {
          name           = "GameScores"
          billing_mode   = "PROVISIONED"
          read_capacity  = 20
          write_capacity = 20
          hash_key       = "UserId"
          range_key      = "GameTitle"
          kms_key_arn    = aws_kms_key.dynamodb.arn

          attribute {
            name = "UserId"
            type = "N"
          }

          attribute {
            name = "GameTitle"
            type = "S"
          }
        }
    steps:
      - Create a Customer Managed Key (CMK) in AWS Key Management Service (KMS)
      - Enable encryption for the DynamoDB table
      - Specify the CMK to be used for encryption in the DynamoDB table configuration
  title: Non-encrypted confidential information - DynamoDB
  vulnerability_id: "409"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Enhancement of traceability and logging capabilities in API Gateway
  solution:
    insecure_code_example:
      description:
        "The above code illustrates an AWS API Gateway setup using Terraform\
        \ without enabling the logging feature. \n\nThe `aws_api_gateway_rest_api`\
        \ block creates an API Gateway REST API. The `aws_api_gateway_stage` block\
        \ creates a stage for the API, in this case, \"prod\" (production). \n\nHowever,\
        \ in this configuration, the logging feature is not enabled. This means that\
        \ the API Gateway will not store any logs of its operations. \n\nLogs are\
        \ crucial for monitoring and debugging purposes. They record the activities\
        \ within your application and can provide useful insights. They can also be\
        \ used by other AWS services like CloudWatch to detect potential system anomalies.\
        \ \n\nWithout the logging feature enabled, you might miss critical information\
        \ about your application's behavior, making it harder to identify and fix\
        \ issues. This lack of traceability is a system vulnerability."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_api_gateway_rest_api" "my_rest_api" {
          name        = "my_rest_api"
          description = "This is my API for demonstration purposes"
        }

        resource "aws_api_gateway_stage" "prod" {
          stage_name    = "prod"
          rest_api_id   = aws_api_gateway_rest_api.my_rest_api.id
          deployment_id = aws_api_gateway_deployment.my_deployment.id
        }
    language: aws
    secure_code_example:
      description:
        "The above code fixes the vulnerability by enabling the logging\
        \ feature for the API Gateway instance. This is done by adding the `access_log_settings`\
        \ block inside the `aws_api_gateway_stage` resource. \n\nThe `access_log_settings`\
        \ block has two properties: `destination_arn` and `format`. \n\n- `destination_arn`\
        \ is set to the ARN of a CloudWatch Logs group where the access logs will\
        \ be stored. This is created using the `aws_cloudwatch_log_group` resource.\
        \ The name of the log group is set to be the name of the API Gateway instance\
        \ followed by `-access-logs`.\n\n- `format` is set to a string that specifies\
        \ the format of the access logs. This string can include a number of different\
        \ variables that provide information about the request and response. In this\
        \ case, the format string includes information such as the source IP of the\
        \ request, the HTTP method used, the route key, the protocol, the status code\
        \ of the response, the length of the response, and the request ID.\n\nBy enabling\
        \ logging and setting a destination for the log files, you can ensure that\
        \ all information about requests to and responses from the API Gateway instance\
        \ is preserved. This information can then be used to detect possible system\
        \ anomalies."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_api_gateway_rest_api" "my_rest_api" {
          name        = "my_rest_api"
          description = "This is my API for demonstration purposes"
        }

        resource "aws_api_gateway_stage" "prod" {
          stage_name    = "prod"
          rest_api_id   = aws_api_gateway_rest_api.my_rest_api.id
          deployment_id = aws_api_gateway_deployment.my_deployment.id

          # Enable logging
          access_log_settings {
            destination_arn = aws_cloudwatch_log_group.example.arn
            format          = "$context.identity.sourceIp - - [$context.requestTime] \"$context.httpMethod $context.routeKey $context.protocol\" $context.status $context.responseLength $context.requestId"
          }
        }

        resource "aws_cloudwatch_log_group" "example" {
          name = "${aws_api_gateway_rest_api.my_rest_api.name}-access-logs"
        }
    steps:
      - Enable the logging feature for the API Gateway instance
      - Set a destination for the log files using AWS CloudWatch or AWS CloudTrail
  title: Traceability Loss - API Gateway
  vulnerability_id: "408"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Secure storage of confidential information in the database
  solution:
    insecure_code_example:
      description: |-
        In the above code, the Terraform script is creating an AWS RDS instance with a publicly accessible MySQL database. The database is configured with a username and password, which are directly written in the script. However, the password is a plain text string, which is a security vulnerability as it can be easily read by anyone who has access to this script.

        Moreover, the database instance is publicly accessible, which means it can be accessed from any IP address, making it susceptible to attacks from anyone on the internet.

        Another point to note is that the data stored in this database is not encrypted. AWS RDS supports encryption at rest for many of its database engines, but it's not being utilized in this script.

        The combination of these factors makes this code vulnerable to attacks where an attacker can gain unauthorized access to the database and any information stored within it.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = true
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code includes two major changes to address the vulnerability:

        1. **Encryption at Rest**: The `storage_encrypted` attribute is set to `true` in the `aws_db_instance` resource. This ensures that the data at rest is encrypted. The `kms_key_id` attribute is also added to specify the AWS Key Management Service (KMS) key identifier for the encryption. Replace the placeholder ARN with your actual KMS key ARN.

        2. **SSL/TLS for Data in Transit**: A new `aws_db_option_group` resource is added. This resource is used to enable SSL for the MySQL database. The `option_name` is set to `MARIADB_ENABLE_SSL` and the value is set to `ON`. This ensures that the data in transit between the application and the database is encrypted.

        Please note that this code does not include the implementation of strong access controls and authentication mechanisms for the database, and the regular update and patch of the database software. These are important steps that should be taken to further secure the database, but they are beyond the scope of this code snippet.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = true
          storage_encrypted    = true
          kms_key_id           = "arn:aws:kms:us-west-2:123456789012:key/abcd1234-a123-456a-a12b-a123b4cd56ef"
        }

        resource "aws_db_option_group" "default" {
          engine_name              = "mysql"
          major_engine_version     = "5.7"
          option_group_description = "Option group for enabling SSL"
          option {
            option_name = "MARIADB_ENABLE_SSL"
            option_settings {
              name  = "MARIADB_ENABLE_SSL"
              value = "ON"
            }
          }
        }
    steps:
      - Enable encryption at rest for the database instance
      - Use SSL/TLS to encrypt data in transit between the application and the database
      - Implement strong access controls and authentication mechanisms for the database
      - Regularly update and patch the database software to address any security vulnerabilities
  title: Non-encrypted confidential information - DB
  vulnerability_id: "246"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS IAM Roles for managing access and permissions in AWS services
    - Usage of AWS IAM Role Policy for defining permissions and access control policies
      for AWS IAM roles.
  need: Restriction of privileges and removal of wildcard usage
  solution:
    insecure_code_example:
      description: |-
        The above Terraform code creates an IAM role policy named "excessive_privileges" in AWS. The policy is associated with an IAM role (referenced as aws_iam_role.example.id in the code).

        The vulnerability arises from the fact that the policy grants all ("*") actions on all ("*") resources. This is defined in the "Statement" section of the policy. This means the IAM role associated with this policy has full permissions to perform any action on any resource in the AWS environment.

        This is a critical security vulnerability as it contradicts the principle of least privilege (PoLP), a security concept in which a user is given the minimum levels of access necessary to complete his/her job functions. By assigning excessive privileges, you increase the potential for damage if the role's credentials are compromised.

        Furthermore, the use of wildcards (*) in IAM policies can lead to unintended permissions being granted. For example, a policy that is intended to allow access to a specific S3 bucket could inadvertently grant access to all S3 buckets if a wildcard is used in the resource ARN.

        In this case, any entity (user, service, application) assuming this IAM role would have unrestricted access to perform any operation (read, write, delete, etc.) on any AWS resource. This could lead to unauthorized data access, data loss, or disruption of critical operations.
      text: |-
        resource "aws_iam_role_policy" "excessive_privileges" {
          name = "excessive_privileges"
          role = aws_iam_role.example.id

          policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "*",
              "Resource": "*",
              "Effect": "Allow"
            }
          ]
        }
        EOF
        }
    language: aws
    secure_code_example:
      description: |-
        The original code had a vulnerability due to the use of wildcards (*) in the 'Action' and 'Resource' fields of the IAM role policy. This means that the role had excessive privileges, being able to perform any action on any resource, which is a security risk.

        The fixed code removes the wildcards and specifies the exact actions and resources that the role should have access to. In this case, the role is only allowed to list the contents of a specific S3 bucket (`s3:ListBucket`) and retrieve objects from it (`s3:GetObject`). The resources are also specified to be only the example_bucket and its contents.

        This way, the role's privileges are limited to only what is necessary, reducing the potential for unauthorized access or actions. It's important to regularly review and audit IAM role policies to ensure they remain up-to-date and do not grant excessive privileges.
      text: |-
        resource "aws_iam_role_policy" "excessive_privileges" {
          name = "excessive_privileges"
          role = aws_iam_role.example.id

          policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": [
                "s3:ListBucket",
                "s3:GetObject"
              ],
              "Resource": [
                "arn:aws:s3:::example_bucket",
                "arn:aws:s3:::example_bucket/*"
              ],
              "Effect": "Allow"
            }
          ]
        }
        EOF
        }
    steps:
      - Review the permissions required by the role and identify the specific actions
        and resources that are necessary.
      - Remove the wildcard (*) from the 'Action' field in the IAM role policy.
      - Update the 'Resource' field in the IAM role policy to specify the specific resources
        that the role should have access to.
      - Ensure that the IAM role policy only grants the necessary privileges to perform
        the required actions.
      - Regularly review and audit the IAM role policies to ensure they remain up-to-date
        and do not have excessive privileges.
  title: Excessive privileges - Wildcards
  vulnerability_id: "325"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Secure access to debug APK files
  solution:
    insecure_code_example:
      description: |-
        This Terraform script is used to create an S3 bucket on AWS and upload a file to it. In this case, the file is a debug APK.

        The vulnerability lies in the access control list (ACL) settings of both the S3 bucket and the object. The `acl` attribute is set to `public-read`, which means that any user, authenticated or not, can read the file. In this case, the debug APK is publicly accessible, which is a security risk.

        The debug version of an application can contain sensitive information, such as keys, passwords, and other details that should not be exposed. It might also have fewer security controls than the production version, making it a potential target for attackers.

        Moreover, the S3 bucket itself is also publicly readable, which could expose other files or information.

        Therefore, the `acl` attribute should be changed to a more restrictive setting to prevent unauthorized access. The debug APK should not be stored in a publicly accessible location.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_s3_bucket" {
          bucket = "my_bucket"
          acl    = "public-read"

          tags = {
            Name        = "My bucket"
            Environment = "Dev"
          }
        }

        resource "aws_s3_bucket_object" {
          bucket = "my_bucket"
          key    = "debug.apk"
          source = "path/to/local/file/debug.apk"
          acl    = "public-read"
        }
    language: aws
    secure_code_example:
      description: |-
        The above code is a Terraform script for AWS infrastructure. It creates an S3 bucket and uploads a debug APK file to it. However, the original code had a vulnerability where the S3 bucket and the debug APK file were publicly accessible, which could lead to unauthorized access.

        The fixed code removes the 'acl' attribute from the 'aws_s3_bucket' and 'aws_s3_bucket_object' resources, which was previously set to 'public-read'. This change prevents public access to the bucket and its objects.

        In addition to this, it's recommended to implement authentication mechanisms, such as AWS IAM roles or access keys, to restrict access to the bucket and its objects. AWS S3 bucket policies can also be used to further control access to the bucket and its objects.

        Lastly, it's important to ensure that the debug APK is not included in the production build and is only accessible to authorized users during development and testing phases. This can be achieved by setting up different environments for development, testing, and production, and using appropriate access controls for each environment.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_s3_bucket" {
          bucket = "my_bucket"

          tags = {
            Name        = "My bucket"
            Environment = "Dev"
          }
        }

        resource "aws_s3_bucket_object" {
          bucket = "my_bucket"
          key    = "debug.apk"
          source = "path/to/local/file/debug.apk"
        }
    steps:
      - Remove the 'acl' attribute from the 'aws_s3_bucket' resource to prevent public
        access to the bucket.
      - Implement authentication mechanisms, such as AWS IAM roles or access keys, to
        restrict access to the bucket and its objects.
      - Consider using AWS S3 bucket policies to further control access to the bucket
        and its objects.
      - Ensure that the debug APK is not included in the production build and is only
        accessible to authorized users during development and testing phases.
  title: Unauthorized access to files - Debug APK
  vulnerability_id: "202"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Secure transmission of client information
  solution:
    insecure_code_example:
      description: |-
        The above code is a Terraform script that sets up an AWS instance and a security group that allows HTTP traffic. The AWS instance is associated with this security group.

        The `aws_security_group` resource creates a new security group that allows inbound traffic on port 80, which is the default port for HTTP. The `cidr_blocks` attribute is set to `0.0.0.0/0`, which means it allows traffic from any IP address.

        The `aws_instance` resource creates a new AWS instance and associates it with the previously created security group via the `vpc_security_group_ids` attribute.

        This code is vulnerable because it uses HTTP, a protocol that does not use encryption. Any data transmitted over this protocol, including potentially sensitive client information, can be captured in plain text. This lack of encryption can lead to data breaches and other security incidents.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "web" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "HelloWorld"
          }
        }

        resource "aws_security_group" "allow_http" {
          name        = "allow_http"
          description = "Allow HTTP inbound traffic"

          ingress {
            from_port   = 80
            to_port     = 80
            protocol    = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }

        resource "aws_instance" "web" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"
          vpc_security_group_ids = [aws_security_group.allow_http.id]

          tags = {
            Name = "HelloWorld"
          }
        }
    language: aws
    secure_code_example:
      description:
        "The updated code now uses HTTPS instead of HTTP for transmitting\
        \ client information. This is achieved by changing the security group configuration\
        \ to allow inbound traffic on port 443 (HTTPS) instead of port 80 (HTTP).\
        \ \n\nThe `aws_security_group` resource is updated to \"allow_https\" and\
        \ the description is changed to \"Allow HTTPS inbound traffic\". The `from_port`\
        \ and `to_port` values are updated to 443, which is the standard port for\
        \ HTTPS.\n\nThe `aws_instance` resource is updated to use the new security\
        \ group \"allow_https\". \n\nPlease note that this code change assumes that\
        \ you have already configured your application to use TLS (Transport Layer\
        \ Security) for secure communication, obtained an SSL/TLS certificate for\
        \ your application domain, and configured your web server to use the SSL/TLS\
        \ certificate for HTTPS connections. \n\nAfter updating the code, you should\
        \ deploy the updated infrastructure to the AWS environment."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "web" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "HelloWorld"
          }
        }

        resource "aws_security_group" "allow_https" {
          name        = "allow_https"
          description = "Allow HTTPS inbound traffic"

          ingress {
            from_port   = 443
            to_port     = 443
            protocol    = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }

        resource "aws_instance" "web" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"
          vpc_security_group_ids = [aws_security_group.allow_https.id]

          tags = {
            Name = "HelloWorld"
          }
        }
    steps:
      - Update the code to use HTTPS instead of HTTP for transmitting client information.
      - Configure the application to use TLS (Transport Layer Security) for secure
        communication.
      - Obtain an SSL/TLS certificate for the application domain.
      - Configure the web server to use the SSL/TLS certificate for HTTPS connections.
      - Update the security group configuration to allow inbound traffic on port
        443 (HTTPS) instead of port 80 (HTTP).
      - Deploy the updated code and infrastructure to the AWS environment.
  title: Use of an insecure channel - HTTP
  vulnerability_id: "372"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_elb for managing and configuring Amazon Web Services Elastic Load
      Balancer
  need: Secure configuration of Elastic Load Balancers
  solution:
    insecure_code_example:
      description:
        "The above Terraform code sets up an Elastic Load Balancer (ELB)\
        \ in AWS. However, it contains a vulnerability due to insecure service configuration.\
        \ \n\nHere's a breakdown of the vulnerability:\n\n1. **Insecure Listener Configuration**:\
        \ The listener is set up to accept HTTP traffic on port 80. This is insecure\
        \ because HTTP traffic is unencrypted, meaning that any data sent over this\
        \ connection could be intercepted and read by an attacker. This could include\
        \ sensitive information like user credentials or personal data.\n\n2. **Security\
        \ Group Open to the World**: The security group attached to the ELB (`sg-0011223344`)\
        \ is not defined in this code, but if it allows traffic from `0.0.0.0/0` (the\
        \ entire internet), that would be a potential security issue. An open security\
        \ group increases the attack surface by allowing any IP address to send traffic\
        \ to the ELB.\n\n3. **No Access Logs**: The configuration does not enable\
        \ access logs. Access logs provide detailed records about requests sent to\
        \ your load balancer, and are a crucial part of monitoring and securing your\
        \ infrastructure.\n\n4. **No Connection Draining**: Connection draining ensures\
        \ that in-flight requests have time to complete when an instance is de-registered\
        \ or becomes unhealthy. Without this, those requests may fail, impacting the\
        \ user experience and potentially leading to data loss.\n\nThis insecure configuration\
        \ can unintentionally increase the attack surface of the company's cloud infrastructure."
      text: |-
        resource "aws_elb" "example" {
          name               = "example-elb"
          availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]

          listener {
            instance_port     = 80
            instance_protocol = "http"
            lb_port           = 80
            lb_protocol       = "http"
          }

          security_groups = ["sg-0011223344"]
          subnets         = ["subnet-01234567"]

          instances = ["i-01234567", "i-76543210"]
        }
    language: aws
    secure_code_example:
      description: |-
        The above code fixes the insecure service configuration vulnerability in the Elastic Load Balancer (ELB) by implementing the following changes:

        1. **HTTPS Protocol**: The instance protocol and load balancer protocol have been updated to use HTTPS instead of HTTP. This ensures that all traffic between the load balancer and the backend instances is encrypted, reducing the risk of data being intercepted or manipulated.

        2. **SSL Certificate**: An SSL certificate is specified for the load balancer. This certificate is used to establish a secure connection with the backend instances. The `ssl_certificate_id` parameter is set to the ARN of the SSL certificate stored in AWS IAM.

        3. **Secure Listener Configuration**: The load balancer port (`lb_port`) is set to 443, which is the standard port for HTTPS traffic. This ensures that the load balancer is using a secure listener configuration.

        4. **Security Groups**: The `security_groups` parameter is set to a specific security group. This should be reviewed and updated as necessary to ensure that only the necessary inbound and outbound traffic is allowed.

        5. **Subnets**: The `subnets` parameter is set to a specific subnet. This should be reviewed and updated as necessary to ensure that the load balancer is only accessible from the desired subnets.

        6. **Regular Monitoring and Updating**: It is recommended to regularly monitor and update the ELB configuration to address any new security recommendations or best practices. This includes updating the SSL certificate as necessary and reviewing the security group and subnet configurations.
      text: |-
        resource "aws_elb" "example" {
          name               = "example-elb"
          availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]

          listener {
            instance_port     = 80
            instance_protocol = "https"
            lb_port           = 443
            lb_protocol       = "https"

            ssl_certificate_id = "arn:aws:iam::123456789012:server-certificate/certName"
          }

          security_groups = ["sg-0011223344"]
          subnets         = ["subnet-01234567"]

          instances = ["i-01234567", "i-76543210"]
        }
    steps:
      - Update the ELB configuration to use HTTPS instead of HTTP for the instance
        protocol and load balancer protocol.
      - Create a custom SSL security policy for the ELB that contains secure ciphers.
      - Ensure that the load balancer is using a secure listener configuration.
      - Review and update the security groups associated with the ELB to ensure proper
        access controls.
      - Verify that the ELB is only accessible from the desired subnets.
      - Regularly monitor and update the ELB configuration to address any new security
        recommendations or best practices.
  title: Insecure service configuration - ELB
  vulnerability_id: "070"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need:
    Enhancement of role-based access control to prevent unauthorized privilege
    escalation
  solution:
    insecure_code_example:
      description:
        "The above Terraform code for AWS infrastructure has a critical\
        \ security vulnerability that could lead to a privilege escalation attack.\
        \ \n\nThe vulnerability lies in the way the IAM user and group are configured.\
        \ The \"aws_iam_user\" resource creates a user with the name \"standard_user\"\
        . This user is meant to have standard privileges and should not have administrative\
        \ access.\n\nHowever, the \"aws_iam_group\" resource creates a group with\
        \ the name \"admin_group\". This group is meant for users with administrative\
        \ privileges.\n\nThe critical part is the \"aws_iam_group_membership\" resource.\
        \ This resource adds the \"standard_user\" to the \"admin_group\". This means\
        \ that even though \"standard_user\" should only have standard privileges,\
        \ they are given administrative privileges because they are part of the \"\
        admin_group\".\n\nThis insecure role configuration allows any user with access\
        \ to the \"standard_user\" account to potentially obtain administrative access\
        \ or privileges, which could lead to unauthorized actions, data breaches,\
        \ or other security incidents."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_user" "standard_user" {
          name = "standard_user"
          path = "/"
        }

        resource "aws_iam_group" "admin_group" {
          name = "admin_group"
          path = "/"
        }

        resource "aws_iam_group_membership" "add_user_to_admin_group" {
          name = "add_user_to_admin_group"

          users = [
            aws_iam_user.standard_user.name,
          ]

          group = aws_iam_group.admin_group.name
        }
    language: aws
    secure_code_example:
      description:
        "The original code was vulnerable to privilege escalation because\
        \ it added a standard user to the admin group, effectively granting them administrative\
        \ privileges. \n\nThe fixed code creates a new group called \"standard_group\"\
        \ and adds the standard user to this group instead. This group is then associated\
        \ with a policy that only allows the necessary permissions for a standard\
        \ user, implementing the principle of least privilege. \n\nThe policy allows\
        \ the standard user to describe EC2 instances but does not grant any other\
        \ permissions. This ensures that the user cannot perform any actions that\
        \ they should not be able to, such as modifying or deleting resources.\n\n\
        This fix also involves creating a new IAM policy and attaching it to the standard\
        \ group. This policy restricts the actions that the standard user can perform,\
        \ further reducing the risk of privilege escalation.\n\nRemember to regularly\
        \ review and audit IAM roles and permissions to ensure they are still necessary\
        \ and appropriate. Enable multi-factor authentication (MFA) for privileged\
        \ accounts and implement strong password policies. Regularly update and patch\
        \ the infrastructure components to protect against known vulnerabilities."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_user" "standard_user" {
          name = "standard_user"
          path = "/"
        }

        resource "aws_iam_group" "standard_group" {
          name = "standard_group"
          path = "/"
        }

        resource "aws_iam_group_membership" "add_user_to_standard_group" {
          name = "add_user_to_standard_group"

          users = [
            aws_iam_user.standard_user.name,
          ]

          group = aws_iam_group.standard_group.name
        }

        resource "aws_iam_policy" "standard_user_policy" {
          name        = "standard_user_policy"
          path        = "/"
          description = "IAM policy for restricting standard user access"

          policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Action": "ec2:Describe*",
              "Resource": "*"
            }
          ]
        }
        EOF
        }

        resource "aws_iam_policy_attachment" "attach_standard_user_policy" {
          name       = "attach_standard_user_policy"
          roles      = [aws_iam_group.standard_group.name]
          policy_arn = aws_iam_policy.standard_user_policy.arn
        }
    steps:
      - Implement the principle of least privilege by assigning only the necessary permissions
        to each user or role.
      - Avoid using default or overly permissive IAM roles.
      - Regularly review and audit IAM roles and permissions to ensure they are still
        necessary and appropriate.
      - Enable multi-factor authentication (MFA) for privileged accounts.
      - Implement strong password policies and enforce regular password rotation.
      - Implement secure access controls and restrict access to sensitive resources.
      - Implement proper logging and monitoring to detect and respond to any unauthorized
        access attempts.
      - Regularly update and patch the infrastructure components to protect against
        known vulnerabilities.
      - Educate users on best practices for security and the importance of protecting
        their credentials.
      - Implement a robust identity and access management (IAM) solution to manage user
        roles and permissions.
  title: Privilege escalation
  vulnerability_id: "005"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
    - Usage of AWS IAM Roles for managing access and permissions in AWS services
    - Usage of AWS Lambda Function for serverless computing and event-driven applications
  need: Enforce separate IAM roles for each Lambda function
  solution:
    insecure_code_example:
      description: |-
        In the above Terraform code, we have defined an AWS IAM role `iam_for_lambda` and two AWS Lambda functions `lambda_function1` and `lambda_function2`. Both of these Lambda functions are using the same IAM role `iam_for_lambda` for execution.

        The issue here is that if we need to add any specific permission to `lambda_function1`, we would add it to the IAM role `iam_for_lambda`. However, since `lambda_function2` is also using the same IAM role, it will also get the same permissions. This can lead to unnecessary access and potential security risks.

        The best practice is to have separate IAM roles for each Lambda function. This way, we can manage the permissions for each function separately, reducing the risk of unnecessary access.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_role" "iam_for_lambda" {
          name = "iam_for_lambda"

          assume_role_policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "sts:AssumeRole",
              "Principal": {
          "Service": "lambda.amazonaws.com"
              },
              "Effect": "Allow",
              "Sid": ""
            }
          ]
        }
        EOF
        }

        resource "aws_lambda_function" "lambda_function1" {
          filename      = "lambda_function_payload.zip"
          function_name = "lambda_function_name1"
          role          = aws_iam_role.iam_for_lambda.arn
          handler       = "exports.test"
        }

        resource "aws_lambda_function" "lambda_function2" {
          filename      = "lambda_function_payload.zip"
          function_name = "lambda_function_name2"
          role          = aws_iam_role.iam_for_lambda.arn
          handler       = "exports.test"
        }
    language: aws
    secure_code_example:
      description:
        "The updated code creates a separate IAM role for each Lambda function.\
        \ This is done by defining two `aws_iam_role` resource blocks, each with a\
        \ unique name (`iam_for_lambda1` and `iam_for_lambda2`). \n\nThe `aws_lambda_function`\
        \ resource blocks are then updated to use the respective IAM role ARNs for\
        \ each Lambda function. This is done by setting the `role` attribute to the\
        \ ARN of the respective IAM role (`aws_iam_role.iam_for_lambda1.arn` for `lambda_function1`\
        \ and `aws_iam_role.iam_for_lambda2.arn` for `lambda_function2`).\n\nThis\
        \ approach ensures that each Lambda function has its own dedicated IAM role,\
        \ allowing for more granular control over permissions. This mitigates the\
        \ risk of permissions being unintentionally shared across multiple functions,\
        \ enhancing the security of the infrastructure."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_role" "iam_for_lambda1" {
          name = "iam_for_lambda1"

          assume_role_policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "sts:AssumeRole",
              "Principal": {
          "Service": "lambda.amazonaws.com"
              },
              "Effect": "Allow",
              "Sid": ""
            }
          ]
        }
        EOF
        }

        resource "aws_lambda_function" "lambda_function1" {
          filename      = "lambda_function_payload.zip"
          function_name = "lambda_function_name1"
          role          = aws_iam_role.iam_for_lambda1.arn
          handler       = "exports.test"
        }

        resource "aws_iam_role" "iam_for_lambda2" {
          name = "iam_for_lambda2"

          assume_role_policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "sts:AssumeRole",
              "Principal": {
          "Service": "lambda.amazonaws.com"
              },
              "Effect": "Allow",
              "Sid": ""
            }
          ]
        }
        EOF
        }

        resource "aws_lambda_function" "lambda_function2" {
          filename      = "lambda_function_payload.zip"
          function_name = "lambda_function_name2"
          role          = aws_iam_role.iam_for_lambda2.arn
          handler       = "exports.test"
        }
    steps:
      - Create a separate IAM role for each Lambda function
      - Update the 'aws_iam_role' resource block to define a unique name for each IAM
        role
      - Update the 'aws_lambda_function' resource blocks to use the respective IAM role
        ARNs for each Lambda function
  title: Serverless - one dedicated IAM role per function
  vulnerability_id: "430"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Implementation of robust monitoring and alerting mechanisms
  solution:
    insecure_code_example:
      description:
        "The above code is a simple Terraform script that provisions an\
        \ EC2 instance in the AWS cloud. The provider block is used to configure the\
        \ named provider, in this case, AWS. The resource block defines one resource\
        \ of type \"aws_instance\". \n\nHowever, this script has a major security\
        \ oversight - it does not implement any form of monitoring or alerting. This\
        \ means that if any critical changes occur in the system, such as access and\
        \ modification of resources or roles creation, there will be no notifications\
        \ or alerts sent to inform the relevant parties. \n\nIn a real-world scenario,\
        \ this lack of monitoring can lead to unauthorized access or changes going\
        \ unnoticed, potentially leading to data leaks, system instability, or other\
        \ serious issues. \n\nTo resolve this vulnerability, it's recommended to implement\
        \ AWS CloudWatch or other similar monitoring tools, and configure alerting\
        \ based on specific events or conditions."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "web" {
          ami           = "ami-0c55b159cbfafe1f0"
          instance_type = "t2.micro"

          tags = {
            Name = "HelloWorld"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The above code is a fixed version of the original vulnerable code. It includes the implementation of AWS CloudWatch and SNS (Simple Notification Service) to monitor and send alerts for critical changes in the system.

        The `aws_cloudwatch_metric_alarm` resource is used to create a CloudWatch alarm that triggers when the CPU utilization of the instance exceeds 80% for two consecutive periods of 60 seconds. The `alarm_actions` attribute is set to the ARN of an SNS topic, which means that when the alarm state changes, a message will be sent to this SNS topic.

        The `aws_sns_topic` resource is used to create an SNS topic named "cpu-alerts". This is where the alarm messages will be sent.

        The `aws_sns_topic_subscription` resource is used to subscribe an email endpoint to the SNS topic. This means that when a message is published to the topic, an email will be sent to the specified email address.

        This solution ensures that alerts are sent in response to critical changes in the system, such as high CPU utilization. It can be extended to cover other types of resources and metrics as needed. Regular reviews and updates of the monitoring configuration are recommended to adapt to changes in the system.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "web" {
          ami           = "ami-0c55b159cbfafe1f0"
          instance_type = "t2.micro"

          tags = {
            Name = "HelloWorld"
          }
        }

        resource "aws_cloudwatch_metric_alarm" "cpu_utilization" {
          alarm_name          = "cpu_utilization"
          comparison_operator = "GreaterThanOrEqualToThreshold"
          evaluation_periods  = "2"
          metric_name         = "CPUUtilization"
          namespace           = "AWS/EC2"
          period              = "60"
          statistic           = "Average"
          threshold           = "80"
          alarm_description   = "This metric checks cpu utilization"
          alarm_actions       = [aws_sns_topic.cpu_alerts.arn]
          dimensions = {
            InstanceId = aws_instance.web.id
          }
        }

        resource "aws_sns_topic" "cpu_alerts" {
          name = "cpu-alerts"
        }

        resource "aws_sns_topic_subscription" "cpu_alerts_email" {
          topic_arn = aws_sns_topic.cpu_alerts.arn
          protocol  = "email"
          endpoint  = "youremail@example.com"
        }
    steps:
      - Implement a monitoring solution to track critical changes in the system
      - Configure alerts or notifications for access and modification of resources
      - Set up monitoring for role creation and modification
      - Ensure that the monitoring solution covers all relevant system resources and
        services
      - Regularly review and update the monitoring configuration to adapt to changes
        in the system
  title: Security controls absence - Monitoring
  vulnerability_id: "399"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS)
  need: Secure configuration of EC2 Security Groups
  solution:
    insecure_code_example:
      description: |-
        In the above Terraform code, an AWS EC2 instance is being created without any security group associated with it. This is a major security vulnerability as it means that the instance is not protected by any firewall rules, allowing unrestricted access from the internet.

        A Security Group acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups.

        In the provided code, there are no input/output rules defined for the EC2 instance, which means anyone can access the instance, making it vulnerable to various types of attacks such as DDoS attacks, Brute Force attacks, etc. This is a serious security concern and should be addressed immediately.
      text: |-
        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          # No security group is defined
        }
    language: aws
    secure_code_example:
      description: |-
        The above code first defines a new AWS security group named "example". This security group has an inbound rule that allows TCP traffic on port 22 (SSH) from any IP address (0.0.0.0/0). The outbound rule allows all traffic to any IP address.

        Then, the code updates the `aws_instance` resource to include the `vpc_security_group_ids` attribute. This attribute is set to the ID of the "example" security group. This means that the EC2 instance will now be associated with this security group, and all traffic to and from the instance will be subject to the rules defined in the security group.

        This change ensures that the EC2 instance has a defined security group, and that the security group has explicit inbound and outbound rules. This reduces the risk of unauthorized access to the EC2 instance.
      text: |-
        resource "aws_security_group" "example" {
          name        = "example"
          description = "Example security group"

          ingress {
            from_port   = 22
            to_port     = 22
            protocol    = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
          }

          egress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          vpc_security_group_ids = [aws_security_group.example.id]
        }
    steps:
      - Identify the required inbound and outbound traffic for the EC2 instance.
      - Create a new security group or use an existing one that meets the requirements.
      - Define the inbound and outbound rules for the security group to allow the necessary
        traffic.
      - Update the 'aws_instance' resource in the Terraform code to include the 'vpc_security_group_ids'
        attribute.
      - Set the 'vpc_security_group_ids' attribute to the ID(s) of the security group(s)
        created or selected in the previous step.
  title: Insecure service configuration - Security Groups
  vulnerability_id: "177"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS KMS Key for encryption and decryption of data
  need: Enforce automatic key rotation for KMS service
  solution:
    insecure_code_example:
      description:
        "The code above represents a Terraform configuration for an AWS\
        \ Key Management Service (KMS) key. The vulnerability lies in the `enable_key_rotation`\
        \ attribute which is set to `false`. \n\nThe `enable_key_rotation` attribute\
        \ determines whether the KMS key rotates every year or not. When the attribute\
        \ is set to `false`, the key does not rotate, meaning it remains static. This\
        \ means that the same key is used over a long period of time which increases\
        \ the chances of the key being compromised.\n\nIn the context of AWS KMS,\
        \ key rotation is a security best practice because it reduces the potential\
        \ impact of a compromised key as data encrypted with a new key cannot be accessed\
        \ with an old key. Therefore, by not enabling key rotation, the infrastructure\
        \ is more susceptible to key compromise which can lead to unauthorized access\
        \ to encrypted data."
      text: |-
        resource "aws_kms_key" "mykey" {
          description             = "This is my KMS key"
          deletion_window_in_days = 10
          enable_key_rotation     = false
        }
    language: aws
    secure_code_example:
      description: |-
        The above code is a fixed version of the vulnerable code. The vulnerability was due to the `enable_key_rotation` attribute being set to `false` in the `aws_kms_key` resource. This means that the KMS key was not being rotated automatically, which could potentially lead to a security risk as static keys over long periods are more likely to be compromised.

        In the fixed code, the `enable_key_rotation` attribute is set to `true`. This means that AWS will automatically rotate the KMS key every year. This greatly reduces the risk of the key being compromised as it is regularly changed.

        This is a best practice for managing AWS KMS keys, as it ensures that keys are not static and are regularly updated, reducing the potential attack surface for malicious actors.
      text: |-
        resource "aws_kms_key" "mykey" {
          description             = "This is my KMS key"
          deletion_window_in_days = 10
          enable_key_rotation     = true
        }
    steps:
      - Enable automatic key rotation for the KMS key
      - Set the 'enable_key_rotation' attribute to 'true' in the 'aws_kms_key' resource
  title: Insecure service configuration - KMS
  vulnerability_id: "396"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Implementation of full disk encryption
  solution:
    insecure_code_example:
      description: |-
        The above code is a simple example of a Terraform script that creates an AWS EC2 instance. The root block device defined here is the main hard drive for the instance. However, this code has a vulnerability - the root block device (or hard drive) is not encrypted.

        The `root_block_device` block defines the settings for the root hard disk. Here, it is set to a General Purpose SSD (`gp2`) with a size of 50GB, and is configured to be deleted when the instance is terminated. However, there is no `encrypted` argument in the `root_block_device` block, which means the data on the hard drive is not encrypted.

        This leaves any data stored on this disk unprotected and can be easily accessed if the disk is compromised. This is a serious issue especially when the instance is storing sensitive customer data.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          root_block_device {
            volume_type = "gp2"
            volume_size = "50"
            delete_on_termination = true
          }
        }
    language: aws
    secure_code_example:
      description:
        "The updated code now includes encryption for the root block device\
        \ of the AWS instance. This is achieved by setting the 'encrypted' property\
        \ to 'true'. \n\nThe 'volume_type' has been changed from 'gp2' to 'gp3' because\
        \ 'gp2' does not support encryption. 'gp3' is the latest generation of General\
        \ Purpose SSD volumes that AWS offers, and it supports encryption.\n\nThe\
        \ 'kms_key_id' property has been added to specify a custom AWS KMS key for\
        \ encryption. Replace the placeholder value with the ARN of your own KMS key.\
        \ If you don't specify a custom KMS key, AWS will use the default KMS key\
        \ for EBS in your account.\n\nBy making these changes, the data stored on\
        \ the root block device of the AWS instance will be encrypted, thereby protecting\
        \ sensitive customer data."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          root_block_device {
            volume_type = "gp3"
            volume_size = "50"
            delete_on_termination = true
            encrypted = true
            kms_key_id = "arn:aws:kms:us-west-2:111122223333:key/abcd1234a1234dea4ec1234567890a12" // replace with your KMS Key ARN
          }
        }
    steps:
      - Enable encryption for the root block device of the AWS instance.
      - Specify the 'encrypted' property as 'true' for the root block device.
      - Ensure that the 'volume_type' is set to a type that supports encryption, such
        as 'gp3' or 'io2'.
      - Consider using a custom AWS KMS key for encryption by specifying the 'kms_key_id'
        property.
  title: Non-encrypted hard drives
  vulnerability_id: "250"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS CloudTrail for logging and monitoring AWS API activity
    - Usage of AWS S3 Bucket for storing and retrieving files
  need: Improved data authenticity validation for Cloudtrail Logs
  solution:
    insecure_code_example:
      description: |-
        In the above Terraform code, a CloudTrail instance is being created. However, the `EnableLogFileValidation` attribute is not set. This attribute is used to enable log file integrity validation, which ensures that the log files have not been tampered with.

        Without this attribute enabled, there is no way to guarantee the integrity of the log files. This could potentially allow an attacker to inject malicious information into the log files without raising any alerts, as there would be no mechanism in place to validate the authenticity of the data.

        In this specific context, the vulnerability lies in the absence of the `EnableLogFileValidation` attribute in the `aws_cloudtrail` resource block. This absence means that the CloudTrail instance created by this code will not have log file validation enabled, making it vulnerable to the risks described above.
      text: |-
        resource "aws_cloudtrail" "example" {
          name                          = "example"
          s3_bucket_name                = aws_s3_bucket.example.bucket
          s3_key_prefix                 = "prefix"
          include_global_service_events = true
        }
    language: aws
    secure_code_example:
      description: |-
        The above code is a fixed version of the vulnerable code. The vulnerability was due to the lack of data authenticity validation in the CloudTrail logs. This was because the `EnableLogFileValidation` attribute was not enabled in the CloudTrail instance.

        In the fixed code, the `EnableLogFileValidation` attribute is set to `true`. This means that the integrity validation will be performed on the CloudTrail logs. This will prevent an attacker from injecting malicious information into the log files without raising any alert.

        The `EnableLogFileValidation` attribute is a security feature provided by AWS. When it is enabled, AWS CloudTrail will validate the integrity of the log files. This is done by generating a hash for each log file and storing the hash values in a separate digest file. The digest file is then signed with a private key to ensure its integrity. When the log files are retrieved, AWS CloudTrail will use the corresponding public key to verify the signature of the digest file. If the signature is valid, AWS CloudTrail will then compare the hash values in the digest file with the hash values of the retrieved log files. If the hash values match, it means that the log files have not been tampered with.

        By enabling the `EnableLogFileValidation` attribute, you can ensure the integrity and authenticity of your CloudTrail logs, thereby enhancing the security of your AWS infrastructure.
      text: |-
        resource "aws_cloudtrail" "example" {
          name                          = "example"
          s3_bucket_name                = aws_s3_bucket.example.bucket
          s3_key_prefix                 = "prefix"
          include_global_service_events = true
          enable_log_file_validation    = true
        }
    steps:
      - Enable the EnableLogFileValidation attribute for the CloudTrail instance
      - Set the EnableLogFileValidation attribute to true
  title: Insufficient data authenticity validation - Cloudtrail Logs
  vulnerability_id: "394"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_s3_bucket for interacting with Amazon S3 buckets
  need: Secure access control for S3 buckets
  solution:
    insecure_code_example:
      description:
        "The above Terraform code is creating an AWS S3 bucket with the\
        \ name \"mybucket\". The ACL (Access Control List) property is set to \"public-read\"\
        , which means the bucket is readable by anyone. This is a security vulnerability\
        \ as it allows unauthorized users to access and download the content of this\
        \ bucket without requiring any authentication. \n\nThe `acl` argument controls\
        \ the permissions of the bucket. When it's set to \"public-read\", it grants\
        \ READ access to any user, authenticated or not. This is a major security\
        \ risk as it can lead to unauthorized data exposure, data loss, or even data\
        \ corruption if an attacker decides to delete or modify the stored information.\
        \ \n\nIn addition, if sensitive data is stored in this bucket, it could lead\
        \ to a data breach, which could have significant legal and financial implications.\
        \ Therefore, it's crucial to restrict the access to this bucket to only those\
        \ who need it."
      text: |-
        resource "aws_s3_bucket" "bucket" {
          bucket = "mybucket"
          acl    = "public-read"
        }
    language: aws
    secure_code_example:
      description: |-
        The above code defines an AWS S3 bucket resource using Terraform. The bucket is named "mybucket". The 'acl' attribute, which was previously set to "public-read", has been removed. This change is made to prevent public access to the bucket.

        To further secure the bucket, you should define an AWS S3 bucket policy that specifies the necessary access controls. This policy should be attached to the bucket to ensure that only authorized users and services can access the bucket's content.

        Here is an example of how you can define a bucket policy:

        ```hcl
        resource "aws_s3_bucket_policy" "bucket_policy" {
          bucket = aws_s3_bucket.bucket.id

          policy = jsonencode({
            Version = "2012-10-17"
            Statement = [
              {
                Effect = "Allow"
                Principal = {
                  AWS = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"
                }
                Action = [
                  "s3:GetObject",
                  "s3:PutObject",
                  "s3:DeleteObject"
                ]
                Resource = [
                  "${aws_s3_bucket.bucket.arn}/*"
                ]
              },
            ]
          })
        }
        ```

        In the above example, the bucket policy allows the root user of the AWS account to perform 'GetObject', 'PutObject', and 'DeleteObject' actions on the objects in the bucket. You should modify this policy according to your specific requirements.

        Remember, it's crucial to ensure that your S3 buckets are not publicly accessible unless it's necessary for your use case. Publicly accessible buckets can lead to unauthorized access, data leakage, and potential data loss.
      text: |-
        resource "aws_s3_bucket" "bucket" {
          bucket = "mybucket"
        }
    steps:
      - Remove the 'acl' attribute from the 'aws_s3_bucket' resource
      - Set the appropriate access control policies for the S3 bucket
      - Ensure that the bucket is not publicly accessible
  title: Unauthorized access to files - S3 Bucket
  vulnerability_id: "203"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Implementation of data backup and recovery mechanisms for DynamoDB instances
  solution:
    insecure_code_example:
      description:
        "This code represents a DynamoDB table creation using Terraform.\
        \ The table is named \"GameScores\" and it has two attributes, \"UserId\"\
        \ (numeric type) and \"GameTitle\" (string type). The capacity for read and\
        \ write operations is set to 20 units. \n\nHowever, this code is vulnerable\
        \ due to the lack of protection against deletion. If the table is accidentally\
        \ deleted, all the data it contains will be lost permanently, as there are\
        \ no recovery points in time configured. \n\nThe `aws_dynamodb_table` resource\
        \ doesn't have any settings for backup or point-in-time recovery (PITR). This\
        \ means that once the table is deleted, there's no way to recover the data.\
        \ This could lead to serious consequences, especially if the table contains\
        \ critical data.\n\nTo fix this vulnerability, it would be necessary to enable\
        \ the point-in-time recovery (PITR) option for the DynamoDB table. This can\
        \ be done by adding the `point_in_time_recovery` block into the `aws_dynamodb_table`\
        \ resource."
      text: |-
        resource "aws_dynamodb_table" "basic-dynamodb-table" {
          name           = "GameScores"
          hash_key       = "UserId"
          range_key      = "GameTitle"
          read_capacity  = 20
          write_capacity = 20

          attribute {
            name = "UserId"
            type = "N"
          }

          attribute {
            name = "GameTitle"
            type = "S"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code includes the `point_in_time_recovery` block with `enabled = true` within the `aws_dynamodb_table` resource. This enables the point-in-time recovery (PITR) feature for the DynamoDB table, which provides continuous backups of your table data for the last 35 days. This feature allows you to restore that table to any point in time during the last 35 days.

        This feature is crucial for protecting against accidental write or delete operations. If any unintended DML operation occurs, you can restore the table to a point in time before the operation took place.

        In addition to enabling PITR, it's also recommended to regularly backup the DynamoDB table to a separate storage solution and implement access controls to prevent unauthorized deletion of the DynamoDB table. These steps provide additional layers of protection for your data.
      text: |-
        resource "aws_dynamodb_table" "basic-dynamodb-table" {
          name           = "GameScores"
          hash_key       = "UserId"
          range_key      = "GameTitle"
          read_capacity  = 20
          write_capacity = 20
          point_in_time_recovery {
            enabled = true
          }

          attribute {
            name = "UserId"
            type = "N"
          }

          attribute {
            name = "GameTitle"
            type = "S"
          }
        }
    steps:
      - Enable point-in-time recovery for the DynamoDB table
      - Configure the recovery window to retain data for a sufficient period of time
      - Regularly backup the DynamoDB table to a separate storage solution
      - Implement access controls to prevent unauthorized deletion of the DynamoDB table
  title: Lack of protection against deletion - DynamoDB
  vulnerability_id: "259"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Enforcement of strong password expiration policy
  solution:
    insecure_code_example:
      description:
        "In the code above, we are defining an AWS IAM user named \"my_user\"\
        \ using Terraform. We also define a login profile for this user with the `aws_iam_user_login_profile`\
        \ resource. \n\nThe `password_reset_required` attribute is set to `true`,\
        \ which means that the user is forced to change their password the next time\
        \ they sign in. \n\nThe `password_length` attribute is set to `14`, which\
        \ defines the minimum length of the password.\n\nHowever, the code does not\
        \ enforce password expiration. This means that the password for the user will\
        \ not automatically expire after a certain period of time, which is a potential\
        \ security vulnerability. \n\nGood security practices suggest that credentials\
        \ should be renewed in 90-day periods. Without a password expiration policy,\
        \ an attacker who has gained access to a user's credentials could continue\
        \ to have access indefinitely, as long as the password remains unchanged."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_user" "user" {
          name = "my_user"
        }

        resource "aws_iam_user_login_profile" "user_profile" {
          user    = aws_iam_user.user.name
          password_reset_required = true
          password_length = 14
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code includes a new resource `aws_iam_account_password_policy` which is used to enforce a strict password policy for the AWS account. This policy includes:

        - `minimum_password_length`: This sets the minimum password length to 14 characters.
        - `require_lowercase_characters`: This ensures that the password includes at least one lowercase character.
        - `require_numbers`: This ensures that the password includes at least one numeric character.
        - `require_uppercase_characters`: This ensures that the password includes at least one uppercase character.
        - `require_symbols`: This ensures that the password includes at least one special character.
        - `allow_users_to_change_password`: This allows users to change their own passwords.
        - `max_password_age`: This sets the maximum password age to 90 days, enforcing password expiration and requiring users to change their passwords every 90 days.
        - `password_reuse_prevention`: This prevents users from reusing their last 10 passwords.

        This policy aligns with good security practices by enforcing strong passwords and regular password changes.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_iam_user" "user" {
          name = "my_user"
        }

        resource "aws_iam_user_login_profile" "user_profile" {
          user    = aws_iam_user.user.name
          password_reset_required = true
          password_length = 14
        }

        resource "aws_iam_account_password_policy" "strict" {
          minimum_password_length        = 14
          require_lowercase_characters   = true
          require_numbers                = true
          require_uppercase_characters   = true
          require_symbols                = true
          allow_users_to_change_password = true
          max_password_age               = 90
          password_reuse_prevention      = 10
        }
    steps:
      - Enable password expiration for the IAM user by setting the `password_reset_required`
        attribute to `true`.
      - Set a password expiration policy for the IAM user to enforce regular password
        renewal.
      - Consider setting a reasonable password length requirement to enhance security.
      - Regularly review and update the credential policies to align with security and
        cloud standards.
  title: Weak credential policy - Password Expiration
  vulnerability_id: "277"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_elb for managing and configuring Amazon Web Services Elastic Load
      Balancer
  need: Enable Deletion Protection for Elastic Load Balancing
  solution:
    insecure_code_example:
      description:
        "The above Terraform code creates an Elastic Load Balancer (ELB)\
        \ in AWS without the deletion protection feature enabled. The ELB is configured\
        \ to listen on port 80 and distribute incoming traffic among instances in\
        \ the availability zones \"us-west-2a\", \"us-west-2b\", and \"us-west-2c\"\
        .\n\nThe vulnerability lies in the absence of the `enable_deletion_protection`\
        \ attribute in the resource block. This means that the ELB can be deleted\
        \ accidentally or maliciously without any safeguards. \n\nIn a real-world\
        \ scenario, this could lead to potential downtime or data loss if the ELB\
        \ is unintentionally removed. This is particularly concerning in a production\
        \ environment where high availability and data integrity are crucial. \n\n\
        In order to mitigate this vulnerability, it is recommended to enable deletion\
        \ protection for the ELB. This can be done by adding the `enable_deletion_protection`\
        \ attribute to the resource block and setting it to `true`. This will prevent\
        \ the ELB from being deleted from the AWS Management Console, CLI, or API\
        \ until the deletion protection is explicitly removed."
      text: |-
        resource "aws_elb" "example" {
          name               = "example"
          availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]

          listener {
            instance_port     = 80
            instance_protocol = "http"
            lb_port           = 80
            lb_protocol       = "http"
          }
        }
    language: aws
    secure_code_example:
      description:
        "The above code is a fixed version of the vulnerable code. The\
        \ vulnerability was due to the lack of deletion protection in the Elastic\
        \ Load Balancer (ELB) configuration. \n\nIn the fixed code, the `deletion_protection`\
        \ attribute has been added to the `aws_elb` resource configuration. This attribute\
        \ is set to `true`, which means that deletion protection is enabled for this\
        \ ELB. \n\nWith deletion protection enabled, the ELB cannot be deleted by\
        \ any user, including the root user. This prevents accidental deletion of\
        \ the ELB, which could lead to service disruption. \n\nTo apply this fix,\
        \ replace the existing `aws_elb` resource configuration in your Terraform\
        \ code with the fixed code, then run `terraform apply` to update the ELB resource.\
        \ \n\nRemember to always enable deletion protection for your ELBs to ensure\
        \ the stability and reliability of your services."
      text: |-
        resource "aws_elb" "example" {
          name               = "example"
          availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]
          deletion_protection = true

          listener {
            instance_port     = 80
            instance_protocol = "http"
            lb_port           = 80
            lb_protocol       = "http"
          }
        }
    steps:
      - Identify the AWS Elastic Load Balancer (ELB) resource in your infrastructure
        code.
      - Check if the 'deletion_protection' attribute is present in the 'attributes'
        section of the ELB resource.
      - If the 'deletion_protection' attribute is not present, add it to the ELB
        resource configuration.
      - Set the value of the 'deletion_protection' attribute to 'true' to enable
        deletion protection.
      - Save and apply the updated infrastructure code to provision or update the
        ELB resource with deletion protection enabled.
  title: Lack of protection against deletion - ELB
  vulnerability_id: "258"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Secure encryption of confidential information stored in EBS volumes
  solution:
    insecure_code_example:
      description:
        "The above Terraform code describes an AWS EBS volume resource\
        \ named \"example\". The volume is created with a size of 40 GiB in the \"\
        us-west-2a\" availability zone. \n\nThe vulnerability lies in the `encrypted`\
        \ attribute, which is set to `false`. This means that the EBS volume is not\
        \ encrypted, and any data stored on this volume is not protected at rest.\
        \ If an attacker gains access to this volume, they can potentially read sensitive\
        \ data without any restrictions. \n\nIn AWS, EBS volumes can store data for\
        \ any instance that is attached to them. This data can include user data,\
        \ application data, and also system data pertaining to the instance. The lack\
        \ of encryption on this EBS volume means all this data is at risk. \n\nAWS\
        \ provides the ability to encrypt EBS volumes to add an additional layer of\
        \ security to your data at rest. The encryption occurs on the servers that\
        \ host EC2 instances, providing encryption of data as it moves between EC2\
        \ instances and EBS storage."
      text: |-
        resource "aws_ebs_volume" "example" {
          availability_zone = "us-west-2a"
          size              = 40
          encrypted         = false
        }
    language: aws
    secure_code_example:
      description: |-
        The above code is a fixed version of the vulnerable code. The vulnerability was that the EBS volume was not encrypted, which could potentially allow an attacker to read sensitive information regarding the launch configuration of EC2 Instances.

        In the fixed code, the `encrypted` property of the `aws_ebs_volume` resource is set to `true`. This means that the EBS volume will be encrypted, preventing unauthorized access to the data it contains.

        The `availability_zone` and `size` properties remain the same as in the original code. The `availability_zone` property specifies the AWS availability zone in which the EBS volume is to be created, and the `size` property specifies the size of the EBS volume in GiBs.

        After updating the code, you should re-deploy or update your infrastructure to apply the changes. You should also verify that the `encrypted` property is set to `true` for all the EBS volumes in your infrastructure.
      text: |-
        resource "aws_ebs_volume" "example" {
          availability_zone = "us-west-2a"
          size              = 40
          encrypted         = true
        }
    steps:
      - Identify all the EBS volumes in your infrastructure that are not encrypted
      - Update the configuration of each EBS volume to set the 'encrypted' property
        to 'true'
      - Verify that the 'encrypted' property is set to 'true' for all the EBS volumes
      - Re-deploy or update your infrastructure to apply the changes
  title: Non-encrypted confidential information - EBS Volumes
  vulnerability_id: "407"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need:
    Protection of business credentials to prevent unauthorized access and data
    breaches
  solution:
    insecure_code_example:
      description:
        "In the above code, AWS credentials (access_key and secret_key)\
        \ are hardcoded directly into the Terraform configuration file. This is a\
        \ serious security vulnerability because anyone who has access to this file\
        \ can now access your AWS resources. \n\nIf this file is stored in a version\
        \ control system (like Git), then the credentials are also now in the version\
        \ control history. This means that even if you remove the credentials from\
        \ the file, anyone who can access old versions of the file can still see the\
        \ credentials.\n\nThis vulnerability can lead to a business information leak\
        \ if the AWS account is used to store sensitive data. Attackers could use\
        \ these credentials to gain access to your AWS resources and steal or corrupt\
        \ your data. \n\nAlso, if the AWS account has permissions to modify infrastructure,\
        \ then attackers could use these credentials to disrupt your services. For\
        \ example, they could delete EC2 instances, modify security groups, or create\
        \ new resources that you get billed for. \n\nIn addition, because AWS access\
        \ keys are tied to a specific user, any actions taken with these credentials\
        \ will appear to come from that user. This could lead to accountability issues\
        \ if unauthorized actions are taken with these credentials."
      text: |-
        provider "aws" {
          access_key = "AKIAIOSFODNN7EXAMPLE"
          secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
          region     = "us-west-2"
        }
    language: aws
    secure_code_example:
      description: |-
        The original code contained hardcoded AWS access keys, which is a security vulnerability as it can lead to unauthorized access if the code is exposed. The updated code removes these hardcoded keys.

        To securely provide your AWS credentials, you should set them through environment variables. AWS SDKs and the AWS CLI will automatically search for access keys set in the following environment variables:

        - `AWS_ACCESS_KEY_ID`
        - `AWS_SECRET_ACCESS_KEY`

        You can set these variables in your shell's profile file (e.g., `~/.bash_profile`, `~/.bashrc`, `~/.zshrc`).

        ```bash
        export AWS_ACCESS_KEY_ID=your_access_key
        export AWS_SECRET_ACCESS_KEY=your_secret_key
        ```

        Remember to replace `your_access_key` and `your_secret_key` with your actual AWS access key and secret key.

        For additional security, consider implementing multi-factor authentication for your AWS account, regularly rotating your access keys, enforcing strong password policies, and regularly reviewing and updating your network security and access controls.

        Never expose your AWS access keys in your code or version control system. If you have previously committed code with your keys, make sure to remove them from the history.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }
    steps:
      - Remove the hardcoded access key and secret key from the code.
      - Store the access key and secret key securely, such as using environment variables
        or a secrets management service.
      - Implement multi-factor authentication for the AWS account.
      - Regularly rotate the access key and secret key.
      - Implement strong password policies for all accounts.
      - Review and update network security and access controls.
      - Regularly review and update security policies regarding the use of credentials.
  title: Business information leak - Credentials
  vulnerability_id: "214"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of terraform-provider-aws for managing AWS resources using Terraform
  need: Enhancement of credential security
  solution:
    insecure_code_example:
      description: |-
        This Terraform script is setting up an AWS provider and an IAM user with access keys. The `access_key` and `secret_key` are hard-coded credentials which are considered weak and insecure as they can be easily discovered and exploited.

        Also, the IAM user `loadbalancer` is created with an access key. This key is associated with a PGP key from keybase, which may not be secure depending on how the keybase account is managed.

        This script does not enforce any password policies such as minimum length, complexity requirements, or rotation policies. This makes the credentials easier to guess, increasing the risk of a successful brute force attack.

        In addition, storing secrets in plain text in your Terraform files is a bad practice. These secrets will be included in every version of your code that is committed to your repository, making it very easy for an attacker who gains access to your repository to also gain access to your AWS account.
      text: |-
        provider "aws" {
          access_key = "AKIAIOSFODNN7EXAMPLE"
          secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
          region     = "us-west-2"
        }

        resource "aws_iam_user" "lb" {
          name = "loadbalancer"
          path = "/system/"
        }

        resource "aws_iam_access_key" "lb" {
          user = aws_iam_user.lb.name
          pgp_key = "keybase:username"
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code now uses a shared credentials file for the AWS provider, which is a more secure way to handle access keys and secret keys. The access key and secret key are no longer hardcoded in the Terraform configuration file, reducing the risk of them being compromised.

        The `aws_iam_user_login_profile` resource is used to create a login profile for the IAM user, which allows the user to log in to the AWS Management Console.

        A password policy is enforced using the `aws_iam_account_password_policy` resource. This policy requires a minimum password length of 14 characters and requires the use of lowercase characters, numbers, uppercase characters, and symbols. It also allows users to change their password.

        This approach significantly increases the complexity of the system credentials, making it much more difficult for an attacker to guess them. It also allows for the regular rotation of access keys and secret keys, further reducing the risk of compromise.
      text: |-
        data "aws_caller_identity" "current" {}

        provider "aws" {
          region = "us-west-2"
          shared_credentials_file = "~/.aws/credentials"
          profile                 = "default"
        }

        resource "aws_iam_user" "lb" {
          name = "loadbalancer"
          path = "/system/"
        }

        resource "aws_iam_access_key" "lb" {
          user = aws_iam_user.lb.name
        }

        resource "aws_iam_user_login_profile" "lb" {
          user    = aws_iam_user.lb.name
          pgp_key = "keybase:username"
        }

        resource "aws_iam_account_password_policy" "strict" {
          minimum_password_length        = 14
          require_lowercase_characters   = true
          require_numbers                = true
          require_uppercase_characters   = true
          require_symbols                = true
          allow_users_to_change_password = true
        }
    steps:
      - Generate strong and complex access key and secret key for the AWS provider.
      - Implement a password policy that enforces strong passwords for the IAM user.
      - Avoid using default or easily guessable values for access key and secret key.
      - Regularly rotate the access key and secret key to minimize the risk of compromise.
      - Consider using a secure key management system to store and manage the access
        key and secret key.
      - Monitor and log failed login attempts to detect and respond to brute force attacks.
  title: Guessed weak credentials
  vulnerability_id: "050"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with AWS services and resources
  need:
    Secure encryption of confidential information in AWS Elastic File System (EFS)
    using Customer Managed Keys (CMKs)
  solution:
    insecure_code_example:
      description:
        "The above code creates an AWS Elastic File System (EFS) instance\
        \ using Terraform. However, the code is vulnerable because it does not specify\
        \ any encryption configuration. By default, AWS EFS instances are encrypted\
        \ with a default KMS key provided by AWS. \n\nThis might be a potential security\
        \ risk because the default KMS key is managed by AWS, not by the customer.\
        \ This means that the customer does not have full control over the encrypted\
        \ information. If the default KMS key is compromised, the data stored in the\
        \ EFS instance could be at risk.\n\nBest practices recommend using Customer\
        \ Managed Keys (CMKs) for encryption. With CMKs, the customer has full control\
        \ over the key management, including key creation, rotation, and deletion.\
        \ This can significantly reduce the risk of exposure of the encrypted information.\
        \ \n\nTo fix this vulnerability, you should specify a KMS key id in the `kms_key_id`\
        \ argument when creating the EFS instance. The KMS key should be a Customer\
        \ Managed Key."
      text: |-
        resource "aws_efs_file_system" "example" {
          creation_token = "my-product"
        }
    language: aws
    secure_code_example:
      description: |-
        The above code first creates a Customer Managed Key (CMK) using the `aws_kms_key` resource. The `description` attribute is used to provide a meaningful name for the key, and `deletion_window_in_days` is set to 7 days. This means that when this key is deleted, AWS will retain the key for an additional 7 days during which you can recover the key if it was deleted by mistake.

        Next, the `aws_efs_file_system` resource is updated to use the newly created CMK for encryption. The `kms_key_id` attribute is set to the ARN of the CMK, and the `encrypted` attribute is set to `true` to enable encryption.

        This change ensures that the EFS file system is encrypted using a CMK, giving you full control over the encryption and decryption of your data. This is a best practice for securing sensitive data in AWS EFS.
      text: |-
        resource "aws_kms_key" "efs_encryption_key" {
          description             = "KMS key for EFS encryption"
          deletion_window_in_days = 7
        }

        resource "aws_efs_file_system" "example" {
          creation_token = "my-product"

          kms_key_id = aws_kms_key.efs_encryption_key.arn
          encrypted  = true
        }
    steps:
      - Create a Customer Managed Key (CMK) in AWS Key Management Service (KMS)
      - Enable encryption for the AWS Elastic File System (EFS) resource
      - Specify the CMK ARN (Amazon Resource Name) in the encryption configuration of
        the EFS resource
  title: Non-encrypted confidential information - EFS
  vulnerability_id: "406"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of terraform-provider-aws for managing AWS resources using Terraform
  need: Secure configuration of EC2 instances
  solution:
    insecure_code_example:
      description: |-
        This Terraform script creates an EC2 instance with a publicly accessible AMI. The instance type is t2.micro and is located in the us-west-2 region. The instance is tagged with the name "example-instance".

        The vulnerability here is that the EC2 instance is publicly accessible, and there are no security groups or key pairs specified for this instance. This means that anyone can access this instance if they have the public IP address, which can be easily obtained.

        Furthermore, the lack of a specified key pair means that anyone who gains access to the instance will have full permissions, including the ability to install, modify, or delete any data or applications on the instance.

        Finally, the lack of security groups means that all inbound and outbound traffic is allowed by default. This makes the instance vulnerable to attacks such as DDoS attacks, brute force attacks, or data exfiltration.

        This configuration is a significant security risk and should not be used in a production environment.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"

          tags = {
            Name = "example-instance"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code includes several security enhancements:

        1. **AMI**: The code now uses a data source to fetch the most recent AMI owned by the account. This ensures that the EC2 instance is launched with an updated and patched AMI.

        2. **Security Group**: A security group is created and associated with the EC2 instance. This security group only allows inbound SSH traffic and allows all outbound traffic. Adjust these rules according to your application's requirements.

        3. **IAM Role**: An IAM role and instance profile are created and associated with the EC2 instance. This limits the permissions of the EC2 instance to only what is defined in the IAM role.

        4. **EBS Encryption**: The root EBS volume of the EC2 instance is encrypted, ensuring that data at rest is secure.

        Remember to replace `"my-ami-name"` with the name of your secure AMI, and adjust the security group rules and IAM role according to your application's requirements.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        data "aws_ami" "example" {
          most_recent = true
          owners      = ["self"]

          filter {
            name   = "name"
            values = ["my-ami-name"]
          }
        }

        resource "aws_instance" "example" {
          ami           = data.aws_ami.example.id
          instance_type = "t2.micro"

          vpc_security_group_ids = [aws_security_group.example.id]

          iam_instance_profile = aws_iam_instance_profile.example.name

          ebs_block_device {
            device_name = "/dev/sda1"
            encrypted   = true
          }

          tags = {
            Name = "example-instance"
          }
        }

        resource "aws_security_group" "example" {
          name        = "example"
          description = "Example security group"

          ingress {
            from_port   = 22
            to_port     = 22
            protocol    = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
          }

          egress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }

        resource "aws_iam_instance_profile" "example" {
          name = "example"
          role = aws_iam_role.example.name
        }

        resource "aws_iam_role" "example" {
          name = "example"

          assume_role_policy = <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Action": "sts:AssumeRole",
              "Principal": {
                "Service": "ec2.amazonaws.com"
              },
              "Effect": "Allow",
              "Sid": ""
            }
          ]
        }
        EOF
        }
    steps:
      - Update the EC2 instance configuration to use a secure AMI (Amazon Machine
        Image) that is regularly updated and patched.
      - Review and update the instance type to ensure it meets the security requirements
        of your application.
      - Implement security groups to restrict inbound and outbound traffic to only
        necessary ports and protocols.
      - Enable VPC (Virtual Private Cloud) flow logs to monitor network traffic and
        detect any suspicious activity.
      - Configure IAM (Identity and Access Management) roles and policies to limit
        access permissions for the EC2 instances.
      - Enable encryption at rest for the EC2 instance storage volumes.
      - Implement regular security updates and patches for the EC2 instances.
      - Implement monitoring and logging solutions to detect and respond to any security
        incidents.
      - Regularly review and audit the EC2 instance configurations to ensure compliance
        with security best practices.
      - Implement automated backups and disaster recovery plans for the EC2 instances
        and their data.
  title: Insecure service configuration - EC2
  vulnerability_id: "333"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Enhancement of authorization controls for web services - RDS
  solution:
    insecure_code_example:
      description: |-
        The above Terraform script creates a MySQL RDS instance on AWS. The `publicly_accessible` parameter is set to `true`, which means this RDS instance is accessible from the internet. This is a significant security risk as it exposes the database to potential unauthorized access. If the credentials for the database were to be compromised (e.g., through a brute force attack), an attacker could gain full access to the data stored in the RDS instance.

        The `aws_db_instance` resource block specifies the properties of the RDS instance. The `publicly_accessible` parameter determines whether the RDS instance has a public IP address. By setting this parameter to `true`, the RDS instance is assigned a public IP address and can be accessed from outside of the VPC, which is not recommended for production environments due to the associated security risks.

        The `username` and `password` parameters specify the credentials used to connect to the RDS instance. In this case, the credentials are hardcoded into the Terraform script, which is also a bad practice as it exposes sensitive information in plain text. This could lead to unauthorized access if the script were to be compromised.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = true
        }
    language: aws
    secure_code_example:
      description:
        "The above code is a Terraform script for creating an AWS RDS instance.\
        \ The vulnerability in the original code was that the RDS instance was publicly\
        \ accessible, which could potentially compromise the stored information. \n\
        \nThe fix for this vulnerability is to set the `publicly_accessible` attribute\
        \ to `false`. This ensures that the RDS instance is not publicly accessible\
        \ over the internet, and can only be accessed within the VPC it is deployed\
        \ in. \n\nIn addition to this, it is also recommended to configure proper\
        \ authentication and authorization mechanisms for accessing the RDS instance.\
        \ This can be done by managing AWS IAM roles and policies, and ensuring that\
        \ only authenticated and authorized users and roles can access the RDS instance.\
        \ \n\nPlease note that the `username` and `password` in the script should\
        \ be replaced with secure values, and should ideally be fetched from a secure\
        \ secret management system instead of being hardcoded in the script."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_db_instance" "default" {
          allocated_storage    = 20
          storage_type         = "gp2"
          engine               = "mysql"
          engine_version       = "5.7"
          instance_class       = "db.t2.micro"
          name                 = "mydb"
          username             = "foo"
          password             = "foobarbaz"
          parameter_group_name = "default.mysql5.7"
          publicly_accessible  = false
        }
    steps:
      - Disable public accessibility for the RDS instance
      - Configure proper authentication and authorization mechanisms for accessing the
        RDS instance
      - Ensure that only authenticated and authorized users and roles can access the
        RDS instance
  title: Improper authorization control for web services - RDS
  vulnerability_id: "073"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Prevention of automatic information enumeration
  solution:
    insecure_code_example:
      description: |-
        The above code is a Terraform script for AWS that creates a security group allowing all inbound and outbound traffic. This is a potential security vulnerability as it can allow for unauthorized access to the system, leading to automatic information enumeration.

        In the `aws_security_group` resource block, the `ingress` and `egress` blocks specify the rules for inbound and outbound traffic respectively. The `from_port` and `to_port` are set to `0`, and the `protocol` is set to `-1`, which means all protocols and ports are allowed. The `cidr_blocks` is set to `0.0.0.0/0`, which means all IP addresses are allowed.

        This configuration essentially leaves the system wide open, making it possible for attackers to automatically enumerate system information such as open ports, available services and users personal data. This can lead to various attacks including data breaches, denial of service attacks, and other forms of system compromise.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_security_group" "allow_all" {
          name        = "allow_all"
          description = "Allow all inbound traffic"

          ingress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
          egress {
            from_port   = 0
            to_port     = 0
            protocol    = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code now implements a more secure AWS security group configuration. The security group, now named "allow_specific", has been configured to allow specific inbound and outbound traffic instead of all traffic.

        In the ingress rule, the from_port and to_port have been set to 22, which is the standard port for SSH. The protocol has been set to "tcp", and the CIDR block has been set to "10.0.0.0/16", which restricts access to the specified IP range.

        In the egress rule, the from_port and to_port have been set to 80, which is the standard port for HTTP. The protocol has been set to "tcp", and the CIDR block has been set to "10.0.0.0/16", which restricts outbound access to the specified IP range.

        This configuration significantly reduces the risk of automatic information enumeration by limiting the ports, protocols, and IP ranges that can access the system. It is recommended to regularly review and update these rules based on the application's requirements.

        Additionally, consider implementing network access control lists (ACLs) for an extra layer of security and control over network traffic. Also, implement monitoring and logging mechanisms to detect and respond to any unauthorized access attempts or suspicious activities.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_security_group" "allow_specific" {
          name        = "allow_specific"
          description = "Allow specific inbound and outbound traffic"

          ingress {
            from_port   = 22
            to_port     = 22
            protocol    = "tcp"
            cidr_blocks = ["10.0.0.0/16"]
          }

          egress {
            from_port   = 80
            to_port     = 80
            protocol    = "tcp"
            cidr_blocks = ["10.0.0.0/16"]
          }
        }
    steps:
      - Implement proper ingress and egress rules in the AWS security group.
      - Restrict the allowed ports and protocols to only those that are necessary
        for the application.
      - Avoid using '0.0.0.0/0' as the CIDR block for both ingress and egress rules.
      - Regularly review and update the security group rules to ensure they are up
        to date and aligned with the application's requirements.
      - Consider implementing network access control lists (ACLs) to provide an additional
        layer of security and control over network traffic.
      - Implement monitoring and logging mechanisms to detect and respond to any
        unauthorized access attempts or suspicious activities.
  title: Automatic information enumeration
  vulnerability_id: "047"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Secure configuration of DynamoDB service
  solution:
    insecure_code_example:
      description: |-
        In the above Terraform script, a DynamoDB table named "VulnerableTable" is being created with a `PROVISIONED` billing mode and a fixed read/write capacity of 20 units. The hash key for this table is set to "ID", and the attribute is also set to "ID" with a type of "N" (Number).

        The vulnerability lies in the configuration of the read and write capacity. This table is set to a fixed read/write capacity, which can lead to throttling if the demand exceeds the provisioned capacity. Throttling can impact the performance of your application and can lead to an unsatisfactory user experience.

        Moreover, the billing mode is set to `PROVISIONED`, which means you pay for a set amount of read and write capacity, regardless if you use it or not. This can lead to unnecessary costs if your actual usage is much less than your provisioned capacity.

        Finally, the table lacks any sort of access control or encryption at rest settings, which can be a security risk. Without proper access control, any user with access to the AWS account could potentially read, write, or delete data in the table. Without encryption at rest, the data stored in the table is vulnerable to unauthorized access.
      text: |-
        resource "aws_dynamodb_table" "vulnerable_table" {
          name           = "VulnerableTable"
          billing_mode   = "PROVISIONED"
          read_capacity  = 20
          write_capacity = 20
          hash_key       = "ID"

          attribute {
            name = "ID"
            type = "N"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The above code represents a secure configuration for an AWS DynamoDB table using Terraform. Here's a breakdown of the changes made:

        1. **Name**: The name of the DynamoDB table is set to "SecureTable". It's important to use meaningful names for your resources.

        2. **Billing Mode**: The billing mode is set to "PAY_PER_REQUEST". This mode is recommended for applications with unpredictable workloads, as it automatically scales throughput capacity to meet workload demands, and charges for the actual reads and writes your application performs.

        3. **Hash Key**: The hash key attribute is set to "ID" and its type is set to "N" (Number). The hash key attribute is the primary key that uniquely identifies each item in a table. It's important to ensure that the hash key attribute is appropriately defined and matches the data type used in your application.

        4. **Server Side Encryption**: Server-side encryption is enabled for the DynamoDB table. This means that DynamoDB will encrypt your data at rest using an AWS managed key, adding an additional layer of security.

        5. **Point In Time Recovery**: Point-in-time recovery is enabled for the DynamoDB table. This helps protect your table data from accidental write or delete operations. With point-in-time recovery, you can restore that table to any point in time during the last 35 days.

        Remember, it's also important to implement fine-grained access control using AWS Identity and Access Management (IAM) roles and policies, and regularly monitor and review the DynamoDB table's configuration to ensure it remains secure and aligned with best practices.
      text: |-
        resource "aws_dynamodb_table" "secure_table" {
          name           = "SecureTable"
          billing_mode   = "PAY_PER_REQUEST"
          hash_key       = "ID"

          attribute {
            name = "ID"
            type = "N"
          }

          server_side_encryption {
            enabled = true
          }

          point_in_time_recovery {
            enabled = true
          }
        }
    steps:
      - Review the AWS DynamoDB documentation and best practices for secure configurations.
      - Set the appropriate billing mode for the DynamoDB table based on your application's
        requirements.
      - Adjust the read and write capacity settings according to the expected workload
        of your application.
      - Ensure that the hash key attribute is appropriately defined and matches the
        data type used in your application.
      - Consider implementing fine-grained access control using AWS Identity and
        Access Management (IAM) roles and policies.
      - Enable encryption at rest for the DynamoDB table to protect data stored in
        the database.
      - Regularly monitor and review the DynamoDB table's configuration to ensure
        it remains secure and aligned with best practices.
  title: Insecure service configuration - DynamoDB
  vulnerability_id: "181"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of AWS SDK for interacting with Amazon Web Services
  need: Enabling termination/removal protection for EC2 instances
  solution:
    insecure_code_example:
      description: |-
        In the above Terraform code, an AWS instance is being created without the `disable_api_termination` attribute. This attribute is used to enable termination protection for an instance. When this attribute is not specified or set to false, the instance can be easily terminated or deleted.

        The `aws_instance` resource block creates an EC2 instance with the specified AMI and instance type. However, it does not include the `disable_api_termination` attribute that is necessary for protection against deletion. This leaves the instance vulnerable to accidental termination or deletion.

        The lack of deletion protection can lead to accidental data loss or service disruption. It's particularly risky in production environments where the accidental deletion of an instance can lead to significant problems.

        To fix this vulnerability, you should set the `disable_api_termination` attribute to `true` in your `aws_instance` resource block. This will enable termination protection for the instance and protect it from being accidentally terminated or deleted.
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"
        }
    language: aws
    secure_code_example:
      description:
        "The above code is a fixed version of the original vulnerable code.\
        \ The vulnerability was that the EC2 instance did not have termination protection\
        \ enabled. This could potentially lead to accidental termination of the instance,\
        \ which could cause loss of data and service disruption.\n\nTo fix this vulnerability,\
        \ the `disable_api_termination` attribute was added to the `aws_instance`\
        \ resource. This attribute is used to enable termination protection for the\
        \ EC2 instance. When this attribute is set to `true`, it prevents the instance\
        \ from being accidentally terminated through the AWS Management Console, CLI,\
        \ and SDKs. \n\nThis is a critical security measure to ensure the integrity\
        \ and availability of the EC2 instance. It is especially important for instances\
        \ that host critical applications or store sensitive data. \n\nPlease note\
        \ that even with termination protection enabled, the instance can still be\
        \ terminated if the `disable_api_termination` attribute is manually set to\
        \ `false` or if the instance is deleted from the AWS Management Console. Therefore,\
        \ it is also important to implement proper access controls and regularly review\
        \ IAM policies to prevent unauthorized changes."
      text: |-
        provider "aws" {
          region = "us-west-2"
        }

        resource "aws_instance" "example" {
          ami           = "ami-0c94855ba95c574c8"
          instance_type = "t2.micro"
          disable_api_termination = true
        }
    steps:
      - Enable termination protection for the EC2 instance
      - Add the 'disable_api_termination' attribute to the 'aws_instance' resource
      - Set the value of 'disable_api_termination' to 'true'
  title: Lack of protection against deletion - EC2
  vulnerability_id: "257"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Enhancement of credential policy for temporary passwords
  solution:
    insecure_code_example:
      description:
        "The above Terraform code creates an IAM user and a login profile\
        \ for the user without enforcing any password policy. The `aws_iam_user_login_profile`\
        \ resource is used to create a login profile for the IAM user, which allows\
        \ the user to sign-in to the AWS Management Console. The password for the\
        \ user is provided by the `pgp_key` argument, which is currently set to a\
        \ static value. \n\nMoreover, the code attaches a policy to the user allowing\
        \ them to change their password (`iam:ChangePassword`). However, the policy\
        \ does not enforce any rules for password strength. This means that users\
        \ can choose weak passwords, which is a security risk. \n\nThis lack of a\
        \ strong credential policy for temporary passwords poses a serious security\
        \ threat as it makes the system susceptible to brute force and password guessing\
        \ attacks. The absence of a password complexity requirement means that users\
        \ could potentially choose easily guessable passwords, thus compromising the\
        \ security of the system."
      text: |-
        resource "aws_iam_user" "example" {
          name = "example_user"
        }

        resource "aws_iam_user_login_profile" "example" {
          user    = aws_iam_user.example.name
          pgp_key = "keybase:some_person_that_exists"
        }

        data "aws_iam_policy_document" "example" {
          statement {
            actions   = ["iam:ChangePassword"]
            resources = [aws_iam_user.example.arn]
          }
        }

        resource "aws_iam_user_policy" "example" {
          name   = "example_policy"
          user   = aws_iam_user.example.name
          policy = data.aws_iam_policy_document.example.json
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code includes a strong password policy for temporary passwords. The `aws_iam_user_login_profile` resource now includes a `password_length` attribute set to 20 and a `password_reset_required` attribute set to true. This ensures that the temporary password is long and complex, and that the user is required to change it upon their next login.

        The `aws_iam_account_password_policy` resource has been added to enforce a strict password policy across the account. This policy includes a minimum password length of 20 characters, requirements for symbols, numbers, uppercase characters, and lowercase characters, and a maximum password age of 7 days. It also prevents password reuse and requires a password change if the password has expired.

        This updated code helps to mitigate the risk of weak temporary passwords by enforcing a strong password policy and requiring users to change their temporary passwords promptly.
      text: |-
        resource "aws_iam_user" "example" {
          name = "example_user"
        }

        resource "aws_iam_user_login_profile" "example" {
          user    = aws_iam_user.example.name
          pgp_key = "keybase:some_person_that_exists"
          password_length = 20
          password_reset_required = true
        }

        data "aws_iam_policy_document" "example" {
          statement {
            actions   = ["iam:ChangePassword"]
            resources = [aws_iam_user.example.arn]
          }
        }

        resource "aws_iam_user_policy" "example" {
          name   = "example_policy"
          user   = aws_iam_user.example.name
          policy = data.aws_iam_policy_document.example.json
        }

        resource "aws_iam_account_password_policy" "strict" {
          minimum_password_length        = 20
          require_symbols                = true
          require_numbers                = true
          require_uppercase_characters   = true
          require_lowercase_characters   = true
          allow_users_to_change_password = true
          max_password_age               = 7
          password_reuse_prevention      = 5
          hard_expiry                    = true
        }
    steps:
      - Implement a strong password policy for temporary passwords.
      - Use a secure method to generate and store temporary passwords.
      - Ensure that temporary passwords are only valid for a limited period of time.
      - Enforce multi-factor authentication for users when they log in with temporary
        passwords.
      - Regularly review and update the temporary password policy to align with best
        practices and industry standards.
  title: Weak credential policy - Temporary passwords
  vulnerability_id: "364"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_lb for managing load balancers in AWS
    - Usage of aws_lb_target_group for managing target groups in AWS Load Balancer
  need:
    Implementation of secure encryption algorithms and disabling insecure TLS
    protocol versions.
  solution:
    insecure_code_example:
      description: |-
        In the above Terraform code, we define a load balancer listener for an AWS application load balancer. The listener is configured to listen on port 443 (HTTPS) and uses the `ELBSecurityPolicy-2015-05` SSL policy.

        The vulnerability lies in the `ssl_policy` that is being used. The `ELBSecurityPolicy-2015-05` policy allows the usage of insecure TLS protocol versions, including TLS 1.0 and 1.1. These versions of the TLS protocol are known to be insecure and susceptible to various attacks, such as BEAST, CRIME, and POODLE.

        This means that the data being transmitted between the client and the server could potentially be intercepted and decrypted by an attacker. This could lead to sensitive information being exposed, such as user credentials, credit card numbers, or other personal data.

        To fix this vulnerability, the `ssl_policy` should be updated to use a more secure policy, such as `ELBSecurityPolicy-TLS-1-2-2017-01`, which only allows the usage of the secure TLS 1.2 protocol version.
      text: |-
        resource "aws_lb_listener" "example" {
          load_balancer_arn = aws_lb.example.arn
          port              = "443"
          protocol          = "HTTPS"
          ssl_policy        = "ELBSecurityPolicy-2015-05"

          default_action {
            type             = "forward"
            target_group_arn = aws_lb_target_group.example.arn
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The above code is a fixed version of the vulnerable code. The vulnerability was that the server allowed the usage of insecure TLS protocol versions. This is a serious security issue as it can allow attackers to exploit the weak encryption and potentially gain unauthorized access to sensitive data.

        The fix involves changing the `ssl_policy` from `ELBSecurityPolicy-2015-05` to `ELBSecurityPolicy-TLS-1-2-2017-01`. This change ensures that the server is now using a secure version of the TLS protocol, specifically TLSv1.2, which is currently considered secure.

        The `ELBSecurityPolicy-TLS-1-2-2017-01` policy is a predefined policy by AWS which includes the secure TLSv1.2 protocol along with secure cipher suites. This policy will help protect the server from attacks that target weak protocols and ciphers.

        Remember, it's important to regularly review and update your security configurations, as what is considered secure evolves over time.
      text: |-
        resource "aws_lb_listener" "example" {
          load_balancer_arn = aws_lb.example.arn
          port              = "443"
          protocol          = "HTTPS"
          ssl_policy        = "ELBSecurityPolicy-TLS-1-2-2017-01"

          default_action {
            type             = "forward"
            target_group_arn = aws_lb_target_group.example.arn
          }
        }
    steps:
      - Update the SSL/TLS protocol version to TLSv1.2 or TLSv1.3
      - Ensure that the SSL/TLS configuration is using secure encryption algorithms
      - Consider using a more secure SSL/TLS policy
  title: Insecure encryption algorithm - SSL/TLS
  vulnerability_id: "016"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws-sdk for interacting with Amazon Web Services (AWS) services
  need: Enforcement of Server-Side Encryption for all S3 buckets
  solution:
    insecure_code_example:
      description:
        "In the above code, an S3 bucket is being created with the name\
        \ \"my-bucket\". The `acl` is set to \"private\", which means that the bucket\
        \ is not public and only certain AWS accounts and users have access to it.\
        \ The `tags` attribute is used to assign metadata to the bucket in the form\
        \ of key-value pairs.\n\nHowever, the issue here is that the `server_side_encryption_configuration`\
        \ attribute is not defined, which means that Server-Side Encryption (SSE)\
        \ is not enabled. SSE is a feature provided by AWS S3 that encrypts data at\
        \ rest, i.e., when the data is stored on disks in AWS data centers.\n\nWithout\
        \ SSE, the data stored in the S3 bucket is not encrypted, making it vulnerable\
        \ to unauthorized access and data breaches. If an attacker somehow gains access\
        \ to the S3 bucket, they could read the unencrypted data directly. \n\nTo\
        \ fix this vulnerability, the `server_side_encryption_configuration` attribute\
        \ should be added to the S3 bucket resource, and the `rule` block should be\
        \ defined with the `apply_server_side_encryption_by_default` attribute, where\
        \ the `sse_algorithm` is set to \"aws:kms\" or \"AES256\"."
      text: |-
        resource "aws_s3_bucket" "bucket" {
          bucket = "my-bucket"
          acl    = "private"

          tags = {
            Name        = "My bucket"
            Environment = "Dev"
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The updated code block now includes the `server_side_encryption_configuration` argument within the `aws_s3_bucket` resource block. This argument is used to enable server-side encryption for the S3 bucket.

        Inside the `server_side_encryption_configuration` block, a `rule` block is defined. This block specifies the encryption rules to be applied to the S3 bucket.

        Within the `rule` block, the `apply_server_side_encryption_by_default` block is defined. This block is used to specify the server-side encryption method to be used by default for all new objects in the bucket.

        The `sse_algorithm` argument inside the `apply_server_side_encryption_by_default` block is set to `"AES256"`. This means that the Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) method is used to encrypt the stored objects.

        The `tags` block remains unchanged. It is used to assign metadata to the S3 bucket in the form of key-value pairs.

        After saving these changes, you can apply the updated Terraform configuration. This will provision the S3 bucket with server-side encryption enabled, ensuring that all stored objects are encrypted at rest. This significantly reduces the risk of data leaks by attackers or unauthorized users.
      text: |-
        resource "aws_s3_bucket" "bucket" {
          bucket = "my-bucket"
          acl    = "private"

          server_side_encryption_configuration {
            rule {
              apply_server_side_encryption_by_default {
                sse_algorithm = "AES256"
              }
            }
          }

          tags = {
            Name        = "My bucket"
            Environment = "Dev"
          }
        }
    steps:
      - Modify the 'aws_s3_bucket' resource block to include the 'server_side_encryption_configuration'
        argument.
      - Set the 'server_side_encryption_configuration' argument to enable server-side
        encryption for the S3 bucket.
      - Choose the appropriate server-side encryption method, such as SSE-S3 or SSE-KMS,
        based on your requirements.
      - Save the changes and apply the updated Terraform configuration to provision
        the S3 bucket with server-side encryption enabled.
  title: Non-encrypted confidential information - S3 Server Side Encryption
  vulnerability_id: "099"
  last_update_time: 09/18/2023
- context:
    - Usage of Terraform for Infrastructure as Code (IaC)
    - Usage of aws_s3_bucket for interacting with Amazon S3 buckets
  need: Implementation of customer-controlled keys for encryption
  solution:
    insecure_code_example:
      description: |-
        In this Terraform code, an S3 bucket is being created with server-side encryption enabled. However, the encryption is configured to use the default AWS KMS key (`alias/aws/s3`). This is an insecure practice as the default keys are managed by AWS and not the customer.

        The `kms_master_key_id` parameter is set to `alias/aws/s3`, which refers to the default AWS-managed KMS key for S3. The `sse_algorithm` parameter is set to `aws:kms`, indicating that AWS KMS is being used for the server-side encryption.

        The problem with this code is that it does not provide the full benefits of the AWS KMS service. Specifically, it does not allow the customer to control and manage their own encryption keys. This can limit the customer's ability to comply with certain security and compliance requirements.

        Furthermore, if the default AWS-managed key is compromised, all data encrypted with that key could be at risk. By contrast, if a customer-managed key is compromised, only the data encrypted with that specific key would be at risk.
      text: |-
        resource "aws_s3_bucket" "bucket" {
          bucket = "my-bucket"
          acl    = "private"

          server_side_encryption_configuration {
            rule {
              apply_server_side_encryption_by_default {
                kms_master_key_id = "alias/aws/s3"
                sse_algorithm     = "aws:kms"
              }
            }
          }
        }
    language: aws
    secure_code_example:
      description: |-
        The above code first creates a Customer Managed Key (CMK) using the `aws_kms_key` resource. The `description` attribute is used to provide a description for the key and `deletion_window_in_days` attribute is used to specify the number of days to retain the key after deletion.

        Then, the `aws_s3_bucket` resource is used to create an S3 bucket with the name "my-bucket". The `acl` attribute is set to "private" to ensure that the bucket is not publicly accessible.

        In the `server_side_encryption_configuration` block, the `rule` block is used to apply server-side encryption by default. The `kms_master_key_id` attribute is set to the ARN of the CMK created earlier, and the `sse_algorithm` attribute is set to "aws:kms" to specify that AWS KMS should be used for encryption.

        This way, instead of using the default AWS managed keys, a customer managed key is used for encryption, which provides more control and security.
      text: |-
        resource "aws_kms_key" "my_key" {
          description             = "My KMS key for S3 bucket encryption"
          deletion_window_in_days = 7
        }

        resource "aws_s3_bucket" "bucket" {
          bucket = "my-bucket"
          acl    = "private"

          server_side_encryption_configuration {
            rule {
              apply_server_side_encryption_by_default {
                kms_master_key_id = aws_kms_key.my_key.arn
                sse_algorithm     = "aws:kms"
              }
            }
          }
        }
    steps:
      - Create a Customer Managed Key (CMK) in AWS Key Management Service (KMS).
      - Replace the 'kms_master_key_id' value with the ARN of the CMK created in
        step 1.
      - Update the 'sse_algorithm' value to 'aws:kms'.
      - Save and apply the changes to the code.
  title: Insecure encryption algorithm - Default encryption
  vulnerability_id: "411"
  last_update_time: 09/18/2023
